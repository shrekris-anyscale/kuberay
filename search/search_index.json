{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 KubeRay \u00b6 KubeRay is an open source toolkit to run Ray applications on Kubernetes. KubeRay provides several tools to improve running and managing Ray's experience on Kubernetes. Ray Operator Backend services to create/delete cluster resources Kubectl plugin/CLI to operate CRD objects Native Job and Serving integration with Clusters Data Scientist centric workspace for fast prototyping (incubating) Kubernetes event dumper for ray clusters/pod/services (future work) Operator Integration with Kubernetes node problem detector (future work) Security \u00b6 If you discover a potential security issue in this project, or think you may have discovered a security issue, we ask that you notify KubeRay Security via our Slack Channel . Please do not create a public GitHub issue. License \u00b6 This project is licensed under the Apache-2.0 License . The Ray docs \u00b6 You can find even more information on deployments of Ray on Kubernetes at the official Ray docs .","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#kuberay","text":"KubeRay is an open source toolkit to run Ray applications on Kubernetes. KubeRay provides several tools to improve running and managing Ray's experience on Kubernetes. Ray Operator Backend services to create/delete cluster resources Kubectl plugin/CLI to operate CRD objects Native Job and Serving integration with Clusters Data Scientist centric workspace for fast prototyping (incubating) Kubernetes event dumper for ray clusters/pod/services (future work) Operator Integration with Kubernetes node problem detector (future work)","title":"KubeRay"},{"location":"#security","text":"If you discover a potential security issue in this project, or think you may have discovered a security issue, we ask that you notify KubeRay Security via our Slack Channel . Please do not create a public GitHub issue.","title":"Security"},{"location":"#license","text":"This project is licensed under the Apache-2.0 License .","title":"License"},{"location":"#the-ray-docs","text":"You can find even more information on deployments of Ray on Kubernetes at the official Ray docs .","title":"The Ray docs"},{"location":"troubleshooting/","text":"Troubleshooting handbook \u00b6 Introduction \u00b6 This page will give you some guild on troubleshooting for some situations when you deploy and use the kuberay. Ray Version Compatibility \u00b6 Problem \u00b6 For every running ray cluster, when we try to connect with the client, we must be careful about the python and ray version we used. There are several issues report failures related to version imcompatibility: #148 , #21549 . Therefore there is a reminder for troubleshooting when come up with that situation. Error cases \u00b6 In the ray client initialization, there are several checks will be executed in ray-project/ray/util/client/__init__.py:L115-L137 . Common cases would be like: ... RuntimeError: Python minor versions differ between client and server: client is 3 .8.10, server is 3 .7.7 or: ... RuntimeError: Client Ray installation incompatible with server: client is 2021 -05-20, server is 2021 -12-07 Some cases may not be so clear: ConnectionAbortedError: Initialization failure from server: Traceback ( most recent call last ) : ... 'AttributeError: ''JobConfig'' object has no attribute ''_parsed_runtime_env' ' Traceback (most recent call last): ... RuntimeError: Version mismatch: The cluster was started with: Ray: 1.9.0 Python: 3.7.7 This process on node NODE_ADDRESS was started with: Ray: 1.10.0 Python: 3.7.7 Solution \u00b6 In above cases, you will need to check if the client ray version is compatible with the images version in the ray cluster's configuration. For example, when you deployed kuberay/ray-operator/config/samples/ray-cluster.mini.yaml , you need to be aware that spec.rayVersion and images version is the same with your expect ray release and same with your ray client version. NOTE: In ray code, the version check will only go through major and minor version, so the python and ray image's minor version match is enough. Also the ray upstream community provide different python version support from 3.6 to 3.9, you can choose the image to match your python version.","title":"Guidance"},{"location":"troubleshooting/#troubleshooting-handbook","text":"","title":"Troubleshooting handbook"},{"location":"troubleshooting/#introduction","text":"This page will give you some guild on troubleshooting for some situations when you deploy and use the kuberay.","title":"Introduction"},{"location":"troubleshooting/#ray-version-compatibility","text":"","title":"Ray Version Compatibility"},{"location":"troubleshooting/#problem","text":"For every running ray cluster, when we try to connect with the client, we must be careful about the python and ray version we used. There are several issues report failures related to version imcompatibility: #148 , #21549 . Therefore there is a reminder for troubleshooting when come up with that situation.","title":"Problem"},{"location":"troubleshooting/#error-cases","text":"In the ray client initialization, there are several checks will be executed in ray-project/ray/util/client/__init__.py:L115-L137 . Common cases would be like: ... RuntimeError: Python minor versions differ between client and server: client is 3 .8.10, server is 3 .7.7 or: ... RuntimeError: Client Ray installation incompatible with server: client is 2021 -05-20, server is 2021 -12-07 Some cases may not be so clear: ConnectionAbortedError: Initialization failure from server: Traceback ( most recent call last ) : ... 'AttributeError: ''JobConfig'' object has no attribute ''_parsed_runtime_env' ' Traceback (most recent call last): ... RuntimeError: Version mismatch: The cluster was started with: Ray: 1.9.0 Python: 3.7.7 This process on node NODE_ADDRESS was started with: Ray: 1.10.0 Python: 3.7.7","title":"Error cases"},{"location":"troubleshooting/#solution","text":"In above cases, you will need to check if the client ray version is compatible with the images version in the ray cluster's configuration. For example, when you deployed kuberay/ray-operator/config/samples/ray-cluster.mini.yaml , you need to be aware that spec.rayVersion and images version is the same with your expect ray release and same with your ray client version. NOTE: In ray code, the version check will only go through major and minor version, so the python and ray image's minor version match is enough. Also the ray upstream community provide different python version support from 3.6 to 3.9, you can choose the image to match your python version.","title":"Solution"},{"location":"best-practice/worker-head-reconnection/","text":"Explanation and Best Practice for workers-head Reconnection \u00b6 Problem \u00b6 For a RayCluster with a head and several workers, if a worker is crashed, it will be relaunched immediately and re-join the same cluster quickly; however, when the head is crashed, it will run into the issue #104 that all worker nodes are lost from the head for a long period of time. Explanation \u00b6 When the head pod was deleted, it will be recreated with a new IP by KubeRay controller\uff0cand the GCS server address is changed accordingly. The Raylets of all workers will try to get GCS address from Redis in \u2018ReconnectGcsServer\u2019, but the redis_clients always use the previous head IP, so they will always fail to get new GCS address. The Raylets will not exit until max retries are reached. There are two configurations determining this long delay: /// The interval at which the gcs rpc client will check if gcs rpc server is ready. RAY_CONFIG(int64_t, ping_gcs_rpc_server_interval_milliseconds, 1000) /// Maximum number of times to retry ping gcs rpc server when gcs server restarts. RAY_CONFIG(int32_t, ping_gcs_rpc_server_max_retries, 600) https://github.com/ray-project/ray/blob/98be9fb5e08befbd6cac3ffbcaa477c5117b0eef/src/ray/gcs/gcs_client/gcs_client.cc#L294-L295 It retries 600 times and each interval is 1s, resulting in total 600s timeout, i.e. 10 min. So immediately after 10-min wait for retries, each client exits and gets restarted while connecting to the new head IP. This issue exists in all stable ray versions (including 1.9.1). This has been reduced to 60s in recent commit in master. Best Practice \u00b6 GCS FT feature #20498 is planned in Ray Core Roadmap. When this feature is released, expect a stable head and GCS such that worker-head connection lost issue will not appear anymore. Before that, to solve the workers-head connection lost, there are two options: Make head more stable: when creating the cluster, allocate sufficient amount of resources on head pod such that it tends to be stable and not easy to crash. You can also set {\"num-cpus\": \"0\"} in \"rayStartParams\" of \"headGroupSpec\" such that Ray scheduler will skip the head node when scheduling workloads. This also helps to maintain the stability of the head. Make reconnection shorter: for version <= 1.9.1, you can set this head param --system-config='{\"ping_gcs_rpc_server_max_retries\": 20}' to reduce the delay from 600s down to 20s before workers reconnect to the new head. Note: we should update this doc when GCS FT feature gets updated.","title":"Worker reconnection"},{"location":"best-practice/worker-head-reconnection/#explanation-and-best-practice-for-workers-head-reconnection","text":"","title":"Explanation and Best Practice for workers-head Reconnection"},{"location":"best-practice/worker-head-reconnection/#problem","text":"For a RayCluster with a head and several workers, if a worker is crashed, it will be relaunched immediately and re-join the same cluster quickly; however, when the head is crashed, it will run into the issue #104 that all worker nodes are lost from the head for a long period of time.","title":"Problem"},{"location":"best-practice/worker-head-reconnection/#explanation","text":"When the head pod was deleted, it will be recreated with a new IP by KubeRay controller\uff0cand the GCS server address is changed accordingly. The Raylets of all workers will try to get GCS address from Redis in \u2018ReconnectGcsServer\u2019, but the redis_clients always use the previous head IP, so they will always fail to get new GCS address. The Raylets will not exit until max retries are reached. There are two configurations determining this long delay: /// The interval at which the gcs rpc client will check if gcs rpc server is ready. RAY_CONFIG(int64_t, ping_gcs_rpc_server_interval_milliseconds, 1000) /// Maximum number of times to retry ping gcs rpc server when gcs server restarts. RAY_CONFIG(int32_t, ping_gcs_rpc_server_max_retries, 600) https://github.com/ray-project/ray/blob/98be9fb5e08befbd6cac3ffbcaa477c5117b0eef/src/ray/gcs/gcs_client/gcs_client.cc#L294-L295 It retries 600 times and each interval is 1s, resulting in total 600s timeout, i.e. 10 min. So immediately after 10-min wait for retries, each client exits and gets restarted while connecting to the new head IP. This issue exists in all stable ray versions (including 1.9.1). This has been reduced to 60s in recent commit in master.","title":"Explanation"},{"location":"best-practice/worker-head-reconnection/#best-practice","text":"GCS FT feature #20498 is planned in Ray Core Roadmap. When this feature is released, expect a stable head and GCS such that worker-head connection lost issue will not appear anymore. Before that, to solve the workers-head connection lost, there are two options: Make head more stable: when creating the cluster, allocate sufficient amount of resources on head pod such that it tends to be stable and not easy to crash. You can also set {\"num-cpus\": \"0\"} in \"rayStartParams\" of \"headGroupSpec\" such that Ray scheduler will skip the head node when scheduling workloads. This also helps to maintain the stability of the head. Make reconnection shorter: for version <= 1.9.1, you can set this head param --system-config='{\"ping_gcs_rpc_server_max_retries\": 20}' to reduce the delay from 600s down to 20s before workers reconnect to the new head. Note: we should update this doc when GCS FT feature gets updated.","title":"Best Practice"},{"location":"components/apiserver/","text":"KubeRay APIServer \u00b6 KubeRay APIServer provides the gRPC and HTTP API to manage kuberay resources. Usage \u00b6 Compute Template \u00b6 Create compute templates in a given namespace \u00b6 POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates { \"name\" : \"default-template\" , \"namespace\" : \"<namespace>\" , \"cpu\" : 2 , \"memory\" : 4 , \"gpu\" : 1 , \"gpuAccelerator\" : \"Tesla-V100\" } List all compute templates in a given namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates { \"compute_templates\" : [ { \"name\" : \"default-template\" , \"namespace\" : \"<namespace>\" , \"cpu\" : 2 , \"memory\" : 4 , \"gpu\" : 1 , \"gpu_accelerator\" : \"Tesla-V100\" } ] } List all compute templates in all namespaces \u00b6 GET {{baseUrl}}/apis/v1alpha2/compute_templates Get compute template by name \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates/<compute_template_name> Delete compute template by name \u00b6 DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates/<compute_template_name> Clusters \u00b6 Create cluster in a given namespace \u00b6 POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters payload { \"name\" : \"test-cluster\" , \"namespace\" : \"<namespace>\" , \"user\" : \"jiaxin.shan\" , \"version\" : \"1.9.2\" , \"environment\" : \"DEV\" , \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"head-template\" , \"image\" : \"ray.io/ray:1.9.2\" , \"serviceType\" : \"NodePort\" , \"rayStartParams\" : {} }, \"workerGroupSpec\" : [ { \"groupName\" : \"small-wg\" , \"computeTemplate\" : \"worker-template\" , \"image\" : \"ray.io/ray:1.9.2\" , \"replicas\" : 2 , \"minReplicas\" : 0 , \"maxReplicas\" : 5 , \"rayStartParams\" : {} } ] } } List all clusters in a given namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters { \"clusters\" : [ { \"name\" : \"test-cluster\" , \"namespace\" : \"<namespace>\" , \"user\" : \"jiaxin.shan\" , \"version\" : \"1.9.2\" , \"environment\" : \"DEV\" , \"cluster_spec\" : { \"head_group_spec\" : { \"compute_template\" : \"head-template\" , \"image\" : \"rayproject/ray:1.9.2\" , \"service_type\" : \"NodePort\" , \"ray_start_params\" : { \"dashboard-host\" : \"0.0.0.0\" , \"node-ip-address\" : \"$MY_POD_IP\" , \"port\" : \"6379\" } }, \"worker_group_spec\" : [ { \"group_name\" : \"small-wg\" , \"compute_template\" : \"worker-template\" , \"image\" : \"rayproject/ray:1.9.2\" , \"replicas\" : 2 , \"min_replicas\" : 0 , \"max_replicas\" : 5 , \"ray_start_params\" : { \"node-ip-address\" : \"$MY_POD_IP\" , } } ] }, \"created_at\" : \"2022-03-13T15:13:09Z\" , \"deleted_at\" : null }, ] } List all clusters in all namespaces \u00b6 GET {{baseUrl}}/apis/v1alpha2/clusters Get cluster by its name and namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters/<cluster_name> Delete cluster by its name and namespace \u00b6 DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters/<cluster_name> RayJob \u00b6 Create ray job in a given namespace \u00b6 POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs payload { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"entrypoint\" : \"string\" , \"metadata\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"runtimeEnv\" : \"string\" , \"jobId\" : \"string\" , \"shutdownAfterJobFinishes\" : true , \"clusterSelector\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"ttlSecondsAfterFinished\" : 0 , \"createdAt\" : \"2022-08-19T21:20:30.494Z\" , \"deleteAt\" : \"2022-08-19T21:20:30.494Z\" , \"jobStatus\" : \"string\" , \"jobDeploymentStatus\" : \"string\" , \"message\" : \"string\" } List all jobs in a given namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs Response { \"jobs\" : [ { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"entrypoint\" : \"string\" , \"metadata\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"runtimeEnv\" : \"string\" , \"jobId\" : \"string\" , \"shutdownAfterJobFinishes\" : true , \"clusterSelector\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"ttlSecondsAfterFinished\" : 0 , \"createdAt\" : \"2022-08-19T21:31:24.352Z\" , \"deleteAt\" : \"2022-08-19T21:31:24.352Z\" , \"jobStatus\" : \"string\" , \"jobDeploymentStatus\" : \"string\" , \"message\" : \"string\" } ] } List all jobs in all namespaces \u00b6 GET {{baseUrl}}/apis/v1alpha2/jobs Get job by its name and namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs/<job_name> Delete job by its name and namespace \u00b6 DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs/<job_name> RayService \u00b6 Create ray service in a given namespace \u00b6 POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services payload { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"serveDeploymentGraphSpec\" : { \"importPath\" : \"string\" , \"runtimeEnv\" : \"string\" , \"serveConfigs\" : [ { \"deploymentName\" : \"string\" , \"replicas\" : 0 , \"routePrefix\" : \"string\" , \"maxConcurrentQueries\" : 0 , \"userConfig\" : \"string\" , \"autoscalingConfig\" : \"string\" , \"actorOptions\" : { \"runtimeEnv\" : \"string\" , \"cpus\" : 0 , \"gpu\" : 0 , \"memory\" : 0 , \"objectStoreMemory\" : 0 , \"resource\" : \"string\" , \"accceleratorType\" : \"string\" } } ] }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"rayServiceStatus\" : { \"applicationStatus\" : \"string\" , \"applicationMessage\" : \"string\" , \"serveDeploymentStatus\" : [ { \"deploymentName\" : \"string\" , \"status\" : \"string\" , \"message\" : \"string\" } ], \"rayServiceEvent\" : [ { \"id\" : \"string\" , \"name\" : \"string\" , \"createdAt\" : \"2022-08-19T21:30:01.097Z\" , \"firstTimestamp\" : \"2022-08-19T21:30:01.097Z\" , \"lastTimestamp\" : \"2022-08-19T21:30:01.097Z\" , \"reason\" : \"string\" , \"message\" : \"string\" , \"type\" : \"string\" , \"count\" : 0 } ], \"rayClusterName\" : \"string\" , \"rayClusterState\" : \"string\" , \"serviceEndpoint\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" } }, \"createdAt\" : \"2022-08-19T21:30:01.097Z\" , \"deleteAt\" : \"2022-08-19T21:30:01.097Z\" } List all services in a given namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services Response \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"serveDeploymentGraphSpec\" : { \"importPath\" : \"string\" , \"runtimeEnv\" : \"string\" , \"serveConfigs\" : [ { \"deploymentName\" : \"string\" , \"replicas\" : 0 , \"routePrefix\" : \"string\" , \"maxConcurrentQueries\" : 0 , \"userConfig\" : \"string\" , \"autoscalingConfig\" : \"string\" , \"actorOptions\" : { \"runtimeEnv\" : \"string\" , \"cpus\" : 0 , \"gpu\" : 0 , \"memory\" : 0 , \"objectStoreMemory\" : 0 , \"resource\" : \"string\" , \"accceleratorType\" : \"string\" } } ] }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"rayServiceStatus\" : { \"applicationStatus\" : \"string\" , \"applicationMessage\" : \"string\" , \"serveDeploymentStatus\" : [ { \"deploymentName\" : \"string\" , \"status\" : \"string\" , \"message\" : \"string\" } ], \"rayServiceEvent\" : [ { \"id\" : \"string\" , \"name\" : \"string\" , \"createdAt\" : \"2022-08-19T21:33:15.485Z\" , \"firstTimestamp\" : \"2022-08-19T21:33:15.485Z\" , \"lastTimestamp\" : \"2022-08-19T21:33:15.485Z\" , \"reason\" : \"string\" , \"message\" : \"string\" , \"type\" : \"string\" , \"count\" : 0 } ], \"rayClusterName\" : \"string\" , \"rayClusterState\" : \"string\" , \"serviceEndpoint\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" } }, \"createdAt\" : \"2022-08-19T21:33:15.485Z\" , \"deleteAt\" : \"2022-08-19T21:33:15.485Z\" } List all services in all namespaces \u00b6 GET {{baseUrl}}/apis/v1alpha2/services Get service by its name and namespace \u00b6 GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services/<service_name> Delete service by its name and namespace \u00b6 DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services/<service_name> Swagger Support \u00b6 Download Swagger UI from Swagger-UI . In this case, we use swagger-ui-3.51.2.tar.gz Unzip package and copy dist folder to third_party folder Use go-bindata to generate go code from static files. mkdir third_party tar -zvxf ~/Downloads/swagger-ui-3.51.2.tar.gz /tmp mv /tmp/swagger-ui-3.51.2/dist third_party/swagger-ui cd apiserver/ go-bindata --nocompress --pkg swagger -o pkg/swagger/datafile.go ./third_party/swagger-ui/...","title":"KubeRay ApiServer"},{"location":"components/apiserver/#kuberay-apiserver","text":"KubeRay APIServer provides the gRPC and HTTP API to manage kuberay resources.","title":"KubeRay APIServer"},{"location":"components/apiserver/#usage","text":"","title":"Usage"},{"location":"components/apiserver/#compute-template","text":"","title":"Compute Template"},{"location":"components/apiserver/#create-compute-templates-in-a-given-namespace","text":"POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates { \"name\" : \"default-template\" , \"namespace\" : \"<namespace>\" , \"cpu\" : 2 , \"memory\" : 4 , \"gpu\" : 1 , \"gpuAccelerator\" : \"Tesla-V100\" }","title":"Create compute templates in a given namespace"},{"location":"components/apiserver/#list-all-compute-templates-in-a-given-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates { \"compute_templates\" : [ { \"name\" : \"default-template\" , \"namespace\" : \"<namespace>\" , \"cpu\" : 2 , \"memory\" : 4 , \"gpu\" : 1 , \"gpu_accelerator\" : \"Tesla-V100\" } ] }","title":"List all compute templates in a given namespace"},{"location":"components/apiserver/#list-all-compute-templates-in-all-namespaces","text":"GET {{baseUrl}}/apis/v1alpha2/compute_templates","title":"List all compute templates in all namespaces"},{"location":"components/apiserver/#get-compute-template-by-name","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates/<compute_template_name>","title":"Get compute template by name"},{"location":"components/apiserver/#delete-compute-template-by-name","text":"DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/compute_templates/<compute_template_name>","title":"Delete compute template by name"},{"location":"components/apiserver/#clusters","text":"","title":"Clusters"},{"location":"components/apiserver/#create-cluster-in-a-given-namespace","text":"POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters payload { \"name\" : \"test-cluster\" , \"namespace\" : \"<namespace>\" , \"user\" : \"jiaxin.shan\" , \"version\" : \"1.9.2\" , \"environment\" : \"DEV\" , \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"head-template\" , \"image\" : \"ray.io/ray:1.9.2\" , \"serviceType\" : \"NodePort\" , \"rayStartParams\" : {} }, \"workerGroupSpec\" : [ { \"groupName\" : \"small-wg\" , \"computeTemplate\" : \"worker-template\" , \"image\" : \"ray.io/ray:1.9.2\" , \"replicas\" : 2 , \"minReplicas\" : 0 , \"maxReplicas\" : 5 , \"rayStartParams\" : {} } ] } }","title":"Create cluster in a given namespace"},{"location":"components/apiserver/#list-all-clusters-in-a-given-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters { \"clusters\" : [ { \"name\" : \"test-cluster\" , \"namespace\" : \"<namespace>\" , \"user\" : \"jiaxin.shan\" , \"version\" : \"1.9.2\" , \"environment\" : \"DEV\" , \"cluster_spec\" : { \"head_group_spec\" : { \"compute_template\" : \"head-template\" , \"image\" : \"rayproject/ray:1.9.2\" , \"service_type\" : \"NodePort\" , \"ray_start_params\" : { \"dashboard-host\" : \"0.0.0.0\" , \"node-ip-address\" : \"$MY_POD_IP\" , \"port\" : \"6379\" } }, \"worker_group_spec\" : [ { \"group_name\" : \"small-wg\" , \"compute_template\" : \"worker-template\" , \"image\" : \"rayproject/ray:1.9.2\" , \"replicas\" : 2 , \"min_replicas\" : 0 , \"max_replicas\" : 5 , \"ray_start_params\" : { \"node-ip-address\" : \"$MY_POD_IP\" , } } ] }, \"created_at\" : \"2022-03-13T15:13:09Z\" , \"deleted_at\" : null }, ] }","title":"List all clusters in a given namespace"},{"location":"components/apiserver/#list-all-clusters-in-all-namespaces","text":"GET {{baseUrl}}/apis/v1alpha2/clusters","title":"List all clusters in all namespaces"},{"location":"components/apiserver/#get-cluster-by-its-name-and-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters/<cluster_name>","title":"Get cluster by its name and namespace"},{"location":"components/apiserver/#delete-cluster-by-its-name-and-namespace","text":"DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/clusters/<cluster_name>","title":"Delete cluster by its name and namespace"},{"location":"components/apiserver/#rayjob","text":"","title":"RayJob"},{"location":"components/apiserver/#create-ray-job-in-a-given-namespace","text":"POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs payload { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"entrypoint\" : \"string\" , \"metadata\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"runtimeEnv\" : \"string\" , \"jobId\" : \"string\" , \"shutdownAfterJobFinishes\" : true , \"clusterSelector\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"ttlSecondsAfterFinished\" : 0 , \"createdAt\" : \"2022-08-19T21:20:30.494Z\" , \"deleteAt\" : \"2022-08-19T21:20:30.494Z\" , \"jobStatus\" : \"string\" , \"jobDeploymentStatus\" : \"string\" , \"message\" : \"string\" }","title":"Create ray job in a given namespace"},{"location":"components/apiserver/#list-all-jobs-in-a-given-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs Response { \"jobs\" : [ { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"entrypoint\" : \"string\" , \"metadata\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"runtimeEnv\" : \"string\" , \"jobId\" : \"string\" , \"shutdownAfterJobFinishes\" : true , \"clusterSelector\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"ttlSecondsAfterFinished\" : 0 , \"createdAt\" : \"2022-08-19T21:31:24.352Z\" , \"deleteAt\" : \"2022-08-19T21:31:24.352Z\" , \"jobStatus\" : \"string\" , \"jobDeploymentStatus\" : \"string\" , \"message\" : \"string\" } ] }","title":"List all jobs in a given namespace"},{"location":"components/apiserver/#list-all-jobs-in-all-namespaces","text":"GET {{baseUrl}}/apis/v1alpha2/jobs","title":"List all jobs in all namespaces"},{"location":"components/apiserver/#get-job-by-its-name-and-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs/<job_name>","title":"Get job by its name and namespace"},{"location":"components/apiserver/#delete-job-by-its-name-and-namespace","text":"DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/jobs/<job_name>","title":"Delete job by its name and namespace"},{"location":"components/apiserver/#rayservice","text":"","title":"RayService"},{"location":"components/apiserver/#create-ray-service-in-a-given-namespace","text":"POST {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services payload { \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"serveDeploymentGraphSpec\" : { \"importPath\" : \"string\" , \"runtimeEnv\" : \"string\" , \"serveConfigs\" : [ { \"deploymentName\" : \"string\" , \"replicas\" : 0 , \"routePrefix\" : \"string\" , \"maxConcurrentQueries\" : 0 , \"userConfig\" : \"string\" , \"autoscalingConfig\" : \"string\" , \"actorOptions\" : { \"runtimeEnv\" : \"string\" , \"cpus\" : 0 , \"gpu\" : 0 , \"memory\" : 0 , \"objectStoreMemory\" : 0 , \"resource\" : \"string\" , \"accceleratorType\" : \"string\" } } ] }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"rayServiceStatus\" : { \"applicationStatus\" : \"string\" , \"applicationMessage\" : \"string\" , \"serveDeploymentStatus\" : [ { \"deploymentName\" : \"string\" , \"status\" : \"string\" , \"message\" : \"string\" } ], \"rayServiceEvent\" : [ { \"id\" : \"string\" , \"name\" : \"string\" , \"createdAt\" : \"2022-08-19T21:30:01.097Z\" , \"firstTimestamp\" : \"2022-08-19T21:30:01.097Z\" , \"lastTimestamp\" : \"2022-08-19T21:30:01.097Z\" , \"reason\" : \"string\" , \"message\" : \"string\" , \"type\" : \"string\" , \"count\" : 0 } ], \"rayClusterName\" : \"string\" , \"rayClusterState\" : \"string\" , \"serviceEndpoint\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" } }, \"createdAt\" : \"2022-08-19T21:30:01.097Z\" , \"deleteAt\" : \"2022-08-19T21:30:01.097Z\" }","title":"Create ray service in a given namespace"},{"location":"components/apiserver/#list-all-services-in-a-given-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services Response \"name\" : \"string\" , \"namespace\" : \"string\" , \"user\" : \"string\" , \"serveDeploymentGraphSpec\" : { \"importPath\" : \"string\" , \"runtimeEnv\" : \"string\" , \"serveConfigs\" : [ { \"deploymentName\" : \"string\" , \"replicas\" : 0 , \"routePrefix\" : \"string\" , \"maxConcurrentQueries\" : 0 , \"userConfig\" : \"string\" , \"autoscalingConfig\" : \"string\" , \"actorOptions\" : { \"runtimeEnv\" : \"string\" , \"cpus\" : 0 , \"gpu\" : 0 , \"memory\" : 0 , \"objectStoreMemory\" : 0 , \"resource\" : \"string\" , \"accceleratorType\" : \"string\" } } ] }, \"clusterSpec\" : { \"headGroupSpec\" : { \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"serviceType\" : \"string\" , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] }, \"workerGroupSpec\" : [ { \"groupName\" : \"string\" , \"computeTemplate\" : \"string\" , \"image\" : \"string\" , \"replicas\" : 0 , \"minReplicas\" : 0 , \"maxReplicas\" : 0 , \"rayStartParams\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" }, \"volumes\" : [ { \"mountPath\" : \"string\" , \"volumeType\" : \"PERSISTENT_VOLUME_CLAIM\" , \"name\" : \"string\" , \"source\" : \"string\" , \"readOnly\" : true , \"hostPathType\" : \"DIRECTORY\" , \"mountPropagationMode\" : \"NONE\" } ] } ] }, \"rayServiceStatus\" : { \"applicationStatus\" : \"string\" , \"applicationMessage\" : \"string\" , \"serveDeploymentStatus\" : [ { \"deploymentName\" : \"string\" , \"status\" : \"string\" , \"message\" : \"string\" } ], \"rayServiceEvent\" : [ { \"id\" : \"string\" , \"name\" : \"string\" , \"createdAt\" : \"2022-08-19T21:33:15.485Z\" , \"firstTimestamp\" : \"2022-08-19T21:33:15.485Z\" , \"lastTimestamp\" : \"2022-08-19T21:33:15.485Z\" , \"reason\" : \"string\" , \"message\" : \"string\" , \"type\" : \"string\" , \"count\" : 0 } ], \"rayClusterName\" : \"string\" , \"rayClusterState\" : \"string\" , \"serviceEndpoint\" : { \"additionalProp1\" : \"string\" , \"additionalProp2\" : \"string\" , \"additionalProp3\" : \"string\" } }, \"createdAt\" : \"2022-08-19T21:33:15.485Z\" , \"deleteAt\" : \"2022-08-19T21:33:15.485Z\" }","title":"List all services in a given namespace"},{"location":"components/apiserver/#list-all-services-in-all-namespaces","text":"GET {{baseUrl}}/apis/v1alpha2/services","title":"List all services in all namespaces"},{"location":"components/apiserver/#get-service-by-its-name-and-namespace","text":"GET {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services/<service_name>","title":"Get service by its name and namespace"},{"location":"components/apiserver/#delete-service-by-its-name-and-namespace","text":"DELETE {{baseUrl}}/apis/v1alpha2/namespaces/<namespace>/services/<service_name>","title":"Delete service by its name and namespace"},{"location":"components/apiserver/#swagger-support","text":"Download Swagger UI from Swagger-UI . In this case, we use swagger-ui-3.51.2.tar.gz Unzip package and copy dist folder to third_party folder Use go-bindata to generate go code from static files. mkdir third_party tar -zvxf ~/Downloads/swagger-ui-3.51.2.tar.gz /tmp mv /tmp/swagger-ui-3.51.2/dist third_party/swagger-ui cd apiserver/ go-bindata --nocompress --pkg swagger -o pkg/swagger/datafile.go ./third_party/swagger-ui/...","title":"Swagger Support"},{"location":"components/cli/","text":"KubeRay CLI \u00b6 KubeRay CLI provides the ability to manage kuberay resources (ray clusters, compute templates etc) through command line interface. Installation \u00b6 Please check release page and download the binaries. Prerequisites \u00b6 Kuberay operator needs to be running. Kuberay apiserver needs to be running and accessible. Development \u00b6 Kuberay CLI uses Cobra framework for the CLI application. Kuberay CLI depends on kuberay apiserver to manage these resources by sending grpc requests to the kuberay apiserver. You can build kuberay binary following this way. cd kuberay/cli go build -o kuberay -a main.go Usage \u00b6 Configure kuberay apiserver endpoint \u00b6 Default kuberay apiserver endpoint: 127.0.0.1:8887 . If kuberay apiserver is not run locally, this must be set in order to manage ray clusters and ray compute templates. Read current kuberay apiserver endpoint \u00b6 ./kuberay config get endpoint Reset kuberay apiserver endpoint to default ( 127.0.0.1:8887 ) \u00b6 ./kuberay config reset endpoint Set kuberay apiserver endpoint \u00b6 ./kuberay config set endpoint <kuberay apiserver endpoint> Manage Ray Clusters \u00b6 Create a Ray Cluster \u00b6 Usage: kuberay cluster create [flags] Flags: --environment string environment of the cluster (valid values: DEV, TESTING, STAGING, PRODUCTION) (default \"DEV\") --head-compute-template string compute template name for ray head --head-image string ray head image --head-service-type string ray head service type (ClusterIP, NodePort, LoadBalancer) (default \"ClusterIP\") --name string name of the cluster -n, --namespace string kubernetes namespace where the cluster will be --user string SSO username of ray cluster creator --version string version of the ray cluster (default \"1.9.0\") --worker-compute-template string compute template name of worker in the first worker group --worker-group-name string first worker group name --worker-image string image of worker in the first worker group --worker-replicas uint32 pod replicas of workers in the first worker group (default 1) Known Limitation: Currently only one worker compute template is supported during creation. Get a Ray Cluster \u00b6 ./kuberay cluster get -n <namespace> <cluster name> List Ray Clusters \u00b6 ./kuberay cluster -n <namespace> list Delete a Ray Cluster \u00b6 ./kuberay cluster delete -n <namespace> <cluster name> Manage Ray Compute Template \u00b6 Create a Compute Template \u00b6 Usage: kuberay template compute create [flags] Flags: --cpu uint32 ray pod CPU (default 1) --gpu uint32 ray head GPU --gpu-accelerator string GPU Accelerator type --memory uint32 ray pod memory in GB (default 1) --name string name of the compute template -n, --namespace string kubernetes namespace where the compute template will be stored Get a Ray Compute Template \u00b6 ./kuberay template compute get -n <namespace> <compute template name> List Ray Compute Templates \u00b6 ./kuberay template compute list -n <namespace> Delete a Ray Compute Template \u00b6 ./kuberay template compute delete -n <namespace> <compute template name> End to end example \u00b6 Configure the endpoints kubectl port-forward svc/kuberay-apiserver-service 8887:8887 -n ray-system ./kuberay config set endpoint 127.0.0.1:8887 Create compute templates ./kuberay template compute create -n <namespace> --cpu 2 --memory 4 --name \"worker-template\" ./kuberay template compute create -n <namespace> --cpu 1 --memory 2 --name \"head-template\" List compute templates created ./kuberay template compute list Create the cluster ./kuberay cluster create -n <namespace> --name test-cluster --user jiaxin.shan \\ --head-compute-template head-template \\ --head-image rayproject/ray:1.9.2 \\ --worker-group-name small-wg \\ --worker-compute-template worker-template \\ --worker-image rayproject/ray:1.9.2 List the clusters ./kuberay cluster list","title":"KubeRay CLI"},{"location":"components/cli/#kuberay-cli","text":"KubeRay CLI provides the ability to manage kuberay resources (ray clusters, compute templates etc) through command line interface.","title":"KubeRay CLI"},{"location":"components/cli/#installation","text":"Please check release page and download the binaries.","title":"Installation"},{"location":"components/cli/#prerequisites","text":"Kuberay operator needs to be running. Kuberay apiserver needs to be running and accessible.","title":"Prerequisites"},{"location":"components/cli/#development","text":"Kuberay CLI uses Cobra framework for the CLI application. Kuberay CLI depends on kuberay apiserver to manage these resources by sending grpc requests to the kuberay apiserver. You can build kuberay binary following this way. cd kuberay/cli go build -o kuberay -a main.go","title":"Development"},{"location":"components/cli/#usage","text":"","title":"Usage"},{"location":"components/cli/#configure-kuberay-apiserver-endpoint","text":"Default kuberay apiserver endpoint: 127.0.0.1:8887 . If kuberay apiserver is not run locally, this must be set in order to manage ray clusters and ray compute templates.","title":"Configure kuberay apiserver endpoint"},{"location":"components/cli/#read-current-kuberay-apiserver-endpoint","text":"./kuberay config get endpoint","title":"Read current kuberay apiserver endpoint"},{"location":"components/cli/#reset-kuberay-apiserver-endpoint-to-default-1270018887","text":"./kuberay config reset endpoint","title":"Reset kuberay apiserver endpoint to default (127.0.0.1:8887)"},{"location":"components/cli/#set-kuberay-apiserver-endpoint","text":"./kuberay config set endpoint <kuberay apiserver endpoint>","title":"Set kuberay apiserver endpoint"},{"location":"components/cli/#manage-ray-clusters","text":"","title":"Manage Ray Clusters"},{"location":"components/cli/#create-a-ray-cluster","text":"Usage: kuberay cluster create [flags] Flags: --environment string environment of the cluster (valid values: DEV, TESTING, STAGING, PRODUCTION) (default \"DEV\") --head-compute-template string compute template name for ray head --head-image string ray head image --head-service-type string ray head service type (ClusterIP, NodePort, LoadBalancer) (default \"ClusterIP\") --name string name of the cluster -n, --namespace string kubernetes namespace where the cluster will be --user string SSO username of ray cluster creator --version string version of the ray cluster (default \"1.9.0\") --worker-compute-template string compute template name of worker in the first worker group --worker-group-name string first worker group name --worker-image string image of worker in the first worker group --worker-replicas uint32 pod replicas of workers in the first worker group (default 1) Known Limitation: Currently only one worker compute template is supported during creation.","title":"Create a Ray Cluster"},{"location":"components/cli/#get-a-ray-cluster","text":"./kuberay cluster get -n <namespace> <cluster name>","title":"Get a Ray Cluster"},{"location":"components/cli/#list-ray-clusters","text":"./kuberay cluster -n <namespace> list","title":"List Ray Clusters"},{"location":"components/cli/#delete-a-ray-cluster","text":"./kuberay cluster delete -n <namespace> <cluster name>","title":"Delete a Ray Cluster"},{"location":"components/cli/#manage-ray-compute-template","text":"","title":"Manage Ray Compute Template"},{"location":"components/cli/#create-a-compute-template","text":"Usage: kuberay template compute create [flags] Flags: --cpu uint32 ray pod CPU (default 1) --gpu uint32 ray head GPU --gpu-accelerator string GPU Accelerator type --memory uint32 ray pod memory in GB (default 1) --name string name of the compute template -n, --namespace string kubernetes namespace where the compute template will be stored","title":"Create a Compute Template"},{"location":"components/cli/#get-a-ray-compute-template","text":"./kuberay template compute get -n <namespace> <compute template name>","title":"Get a Ray Compute Template"},{"location":"components/cli/#list-ray-compute-templates","text":"./kuberay template compute list -n <namespace>","title":"List Ray Compute Templates"},{"location":"components/cli/#delete-a-ray-compute-template","text":"./kuberay template compute delete -n <namespace> <compute template name>","title":"Delete a Ray Compute Template"},{"location":"components/cli/#end-to-end-example","text":"Configure the endpoints kubectl port-forward svc/kuberay-apiserver-service 8887:8887 -n ray-system ./kuberay config set endpoint 127.0.0.1:8887 Create compute templates ./kuberay template compute create -n <namespace> --cpu 2 --memory 4 --name \"worker-template\" ./kuberay template compute create -n <namespace> --cpu 1 --memory 2 --name \"head-template\" List compute templates created ./kuberay template compute list Create the cluster ./kuberay cluster create -n <namespace> --name test-cluster --user jiaxin.shan \\ --head-compute-template head-template \\ --head-image rayproject/ray:1.9.2 \\ --worker-group-name small-wg \\ --worker-compute-template worker-template \\ --worker-image rayproject/ray:1.9.2 List the clusters ./kuberay cluster list","title":"End to end example"},{"location":"components/operator/","text":"Ray Kubernetes Operator \u00b6 KubeRay operator makes deploying and managing Ray clusters on top of Kubernetes painless - clusters are defined as a custom RayCluster resource and managed by a fault-tolerant Ray controller. The Ray Operator is a Kubernetes operator to automate provisioning, management, autoscaling and operations of Ray clusters deployed to Kubernetes. Some of the main features of the operator are: - Management of first-class RayClusters via a custom resource . - Support for heterogenous worker types in a single Ray cluster. - Built-in monitoring via Prometheus. - Use of PodTemplate to create Ray pods - Updated status based on the running pods - Events added to the RayCluster instance - Automatically populate environment variables in the containers - Automatically prefix your container command with the ray start command - Automatically adding the volumeMount at /dev/shm for shared memory - Use of ScaleStartegy to remove specific nodes in specific groups Overview \u00b6 When deployed, the ray operator will watch for K8s events (create/delete/update) for the raycluster resources. The ray operator can create a raycluster (head + multipe workers), delete a cluster, or update the cluster by adding or removing worker pods. Ray cluster creation \u00b6 Once a raycluster resource is created, the operator will configure and create the ray-head and the ray-workers specified in the raycluster manifest as shown below. Ray cluster Update \u00b6 You can update the number of replicas in a worker goup, and specify which exact replica to remove by updated the raycluster resource manifest: Ray cluster example code \u00b6 An example ray code is defined in this configmap that is mounted into the ray head-pod. By examining the logs of the head pod, we can see the list of the IP addresses of the nodes that joined the ray cluster: Deploy the operator \u00b6 kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.3.0&timeout=90s\" Check that the controller is running. $ kubectl get deployments -n ray-system NAME READY UP-TO-DATE AVAILABLE AGE ray-operator 1 /1 1 1 40s $ kubectl get pods -n ray-system NAME READY STATUS RESTARTS AGE ray-operator-75dbbf8587-5lrvn 1 /1 Running 0 31s Delete the operator kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\" Running an example cluster \u00b6 We include a few example config files to deploy RayClusters: Sample Description ray-cluster.mini.yaml Small example consisting of 1 head pod. ray-cluster.heterogeneous.yaml Example with heterogenous worker types. 1 head pod and 2 worker pods, each of which has a different resource quota. ray-cluster.complete.yaml Shows all available custom resource properties. ray-cluster.autoscaler.yaml Shows all available custom resource properties and demonstrates autoscaling. ray-cluster.complete.large.yaml Demonstrates resource configuration for production use-cases. ray-cluster.autoscaler.large.yaml Demonstrates resource configuration for autoscaling Ray clusters in production. Note For production use-cases, make sure to allocate sufficient resources for your Ray pods; it usually makes sense to run one large Ray pod per Kubernetes node. See ray-cluster.complete.large.yaml and ray-cluster.autoscaler.large.yaml for guidance. The rest of the sample configs above are geared towards experimentation in local kind or minikube environments. # Create a RayCluster and a ConfigMap with hello world Ray code. $ kubectl create -f config/samples/ray-cluster.heterogeneous.yaml configmap/ray-code created raycluster.ray.io/raycluster-heterogeneous created # List running clusters. $ kubectl get rayclusters NAME AGE raycluster-heterogeneous 2m48s # The created cluster should include a head pod, worker pod, and a head service. $ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-heterogeneous-head-9t28q 1 /1 Running 0 97s raycluster-heterogeneous-worker-medium-group-l9x9n 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-hldxz 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-tmgtq 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-zc5dh 1 /1 Running 0 97s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 22h raycluster-heterogeneous-head-svc ClusterIP 10 .96.47.129 <none> 6379 /TCP,8265/TCP,10001/TCP 2m18s # check the logs of the head pod $ kubectl logs raycluster-heterogeneous-head-5r6qr 2022 -09-21 13 :21:57,505 INFO usage_lib.py:479 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add ` --disable-usage-stats ` to the command that starts the cluster, or run the following command: ` ray disable-usage-stats ` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details. 2022 -09-21 13 :21:57,505 INFO scripts.py:719 -- Local node IP: 10 .244.0.144 2022 -09-21 13 :22:00,513 SUCC scripts.py:756 -- -------------------- 2022 -09-21 13 :22:00,514 SUCC scripts.py:757 -- Ray runtime started. 2022 -09-21 13 :22:00,514 SUCC scripts.py:758 -- -------------------- 2022 -09-21 13 :22:00,514 INFO scripts.py:760 -- Next steps 2022 -09-21 13 :22:00,514 INFO scripts.py:761 -- To connect to this Ray runtime from another node, run 2022 -09-21 13 :22:00,514 INFO scripts.py:766 -- ray start --address = '10.244.0.144:6379' 2022 -09-21 13 :22:00,514 INFO scripts.py:780 -- Alternatively, use the following Python code: 2022 -09-21 13 :22:00,514 INFO scripts.py:782 -- import ray 2022 -09-21 13 :22:00,514 INFO scripts.py:795 -- ray.init ( address = 'auto' , _node_ip_address = '10.244.0.144' ) 2022 -09-21 13 :22:00,515 INFO scripts.py:799 -- To connect to this Ray runtime from outside of the cluster, for example to 2022 -09-21 13 :22:00,515 INFO scripts.py:803 -- connect to a remote cluster from your laptop directly, use the following 2022 -09-21 13 :22:00,515 INFO scripts.py:806 -- Python code: 2022 -09-21 13 :22:00,515 INFO scripts.py:808 -- import ray 2022 -09-21 13 :22:00,515 INFO scripts.py:814 -- ray.init ( address = 'ray://<head_node_ip_address>:10001' ) 2022 -09-21 13 :22:00,515 INFO scripts.py:820 -- If connection fails, check your firewall settings and network configuration. 2022 -09-21 13 :22:00,515 INFO scripts.py:826 -- To terminate the Ray runtime, run 2022 -09-21 13 :22:00,515 INFO scripts.py:827 -- ray stop 2022 -09-21 13 :22:00,515 INFO scripts.py:905 -- --block 2022 -09-21 13 :22:00,515 INFO scripts.py:907 -- This command will now block forever until terminated by a signal. 2022 -09-21 13 :22:00,515 INFO scripts.py:910 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported. Execute hello world Ray code $ kubectl exec raycluster-heterogeneous-head-9t28q -- python /opt/sample_code.py 2022 -09-21 13 :28:41,176 INFO worker.py:1224 -- Using address 127 .0.0.1:6379 set in the environment variable RAY_ADDRESS 2022 -09-21 13 :28:41,176 INFO worker.py:1333 -- Connecting to existing Ray cluster at address: 10 .244.0.144:6379... 2022 -09-21 13 :28:41,183 INFO worker.py:1515 -- Connected to Ray cluster. View the dashboard at http://10.244.0.144:8265 trying to connect to Ray! now executing some code with Ray! Ray Nodes: { '10.244.0.145' , '10.244.0.143' , '10.244.0.146' , '10.244.0.144' , '10.244.0.147' } Execution time = 4 .855740308761597 The output of hello world Ray code show 5 nodes in the Ray cluster Ray Nodes: {'10.244.0.145', '10.244.0.143', '10.244.0.146', '10.244.0.144', '10.244.0.147'} # Delete the cluster. $ kubectl delete raycluster raycluster-heterogeneous","title":"KubeRay Operator"},{"location":"components/operator/#ray-kubernetes-operator","text":"KubeRay operator makes deploying and managing Ray clusters on top of Kubernetes painless - clusters are defined as a custom RayCluster resource and managed by a fault-tolerant Ray controller. The Ray Operator is a Kubernetes operator to automate provisioning, management, autoscaling and operations of Ray clusters deployed to Kubernetes. Some of the main features of the operator are: - Management of first-class RayClusters via a custom resource . - Support for heterogenous worker types in a single Ray cluster. - Built-in monitoring via Prometheus. - Use of PodTemplate to create Ray pods - Updated status based on the running pods - Events added to the RayCluster instance - Automatically populate environment variables in the containers - Automatically prefix your container command with the ray start command - Automatically adding the volumeMount at /dev/shm for shared memory - Use of ScaleStartegy to remove specific nodes in specific groups","title":"Ray Kubernetes Operator"},{"location":"components/operator/#overview","text":"When deployed, the ray operator will watch for K8s events (create/delete/update) for the raycluster resources. The ray operator can create a raycluster (head + multipe workers), delete a cluster, or update the cluster by adding or removing worker pods.","title":"Overview"},{"location":"components/operator/#ray-cluster-creation","text":"Once a raycluster resource is created, the operator will configure and create the ray-head and the ray-workers specified in the raycluster manifest as shown below.","title":"Ray cluster creation"},{"location":"components/operator/#ray-cluster-update","text":"You can update the number of replicas in a worker goup, and specify which exact replica to remove by updated the raycluster resource manifest:","title":"Ray cluster Update"},{"location":"components/operator/#ray-cluster-example-code","text":"An example ray code is defined in this configmap that is mounted into the ray head-pod. By examining the logs of the head pod, we can see the list of the IP addresses of the nodes that joined the ray cluster:","title":"Ray cluster example code"},{"location":"components/operator/#deploy-the-operator","text":"kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.3.0&timeout=90s\" Check that the controller is running. $ kubectl get deployments -n ray-system NAME READY UP-TO-DATE AVAILABLE AGE ray-operator 1 /1 1 1 40s $ kubectl get pods -n ray-system NAME READY STATUS RESTARTS AGE ray-operator-75dbbf8587-5lrvn 1 /1 Running 0 31s Delete the operator kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\"","title":"Deploy the operator"},{"location":"components/operator/#running-an-example-cluster","text":"We include a few example config files to deploy RayClusters: Sample Description ray-cluster.mini.yaml Small example consisting of 1 head pod. ray-cluster.heterogeneous.yaml Example with heterogenous worker types. 1 head pod and 2 worker pods, each of which has a different resource quota. ray-cluster.complete.yaml Shows all available custom resource properties. ray-cluster.autoscaler.yaml Shows all available custom resource properties and demonstrates autoscaling. ray-cluster.complete.large.yaml Demonstrates resource configuration for production use-cases. ray-cluster.autoscaler.large.yaml Demonstrates resource configuration for autoscaling Ray clusters in production. Note For production use-cases, make sure to allocate sufficient resources for your Ray pods; it usually makes sense to run one large Ray pod per Kubernetes node. See ray-cluster.complete.large.yaml and ray-cluster.autoscaler.large.yaml for guidance. The rest of the sample configs above are geared towards experimentation in local kind or minikube environments. # Create a RayCluster and a ConfigMap with hello world Ray code. $ kubectl create -f config/samples/ray-cluster.heterogeneous.yaml configmap/ray-code created raycluster.ray.io/raycluster-heterogeneous created # List running clusters. $ kubectl get rayclusters NAME AGE raycluster-heterogeneous 2m48s # The created cluster should include a head pod, worker pod, and a head service. $ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-heterogeneous-head-9t28q 1 /1 Running 0 97s raycluster-heterogeneous-worker-medium-group-l9x9n 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-hldxz 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-tmgtq 1 /1 Running 0 97s raycluster-heterogeneous-worker-small-group-zc5dh 1 /1 Running 0 97s $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 22h raycluster-heterogeneous-head-svc ClusterIP 10 .96.47.129 <none> 6379 /TCP,8265/TCP,10001/TCP 2m18s # check the logs of the head pod $ kubectl logs raycluster-heterogeneous-head-5r6qr 2022 -09-21 13 :21:57,505 INFO usage_lib.py:479 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add ` --disable-usage-stats ` to the command that starts the cluster, or run the following command: ` ray disable-usage-stats ` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details. 2022 -09-21 13 :21:57,505 INFO scripts.py:719 -- Local node IP: 10 .244.0.144 2022 -09-21 13 :22:00,513 SUCC scripts.py:756 -- -------------------- 2022 -09-21 13 :22:00,514 SUCC scripts.py:757 -- Ray runtime started. 2022 -09-21 13 :22:00,514 SUCC scripts.py:758 -- -------------------- 2022 -09-21 13 :22:00,514 INFO scripts.py:760 -- Next steps 2022 -09-21 13 :22:00,514 INFO scripts.py:761 -- To connect to this Ray runtime from another node, run 2022 -09-21 13 :22:00,514 INFO scripts.py:766 -- ray start --address = '10.244.0.144:6379' 2022 -09-21 13 :22:00,514 INFO scripts.py:780 -- Alternatively, use the following Python code: 2022 -09-21 13 :22:00,514 INFO scripts.py:782 -- import ray 2022 -09-21 13 :22:00,514 INFO scripts.py:795 -- ray.init ( address = 'auto' , _node_ip_address = '10.244.0.144' ) 2022 -09-21 13 :22:00,515 INFO scripts.py:799 -- To connect to this Ray runtime from outside of the cluster, for example to 2022 -09-21 13 :22:00,515 INFO scripts.py:803 -- connect to a remote cluster from your laptop directly, use the following 2022 -09-21 13 :22:00,515 INFO scripts.py:806 -- Python code: 2022 -09-21 13 :22:00,515 INFO scripts.py:808 -- import ray 2022 -09-21 13 :22:00,515 INFO scripts.py:814 -- ray.init ( address = 'ray://<head_node_ip_address>:10001' ) 2022 -09-21 13 :22:00,515 INFO scripts.py:820 -- If connection fails, check your firewall settings and network configuration. 2022 -09-21 13 :22:00,515 INFO scripts.py:826 -- To terminate the Ray runtime, run 2022 -09-21 13 :22:00,515 INFO scripts.py:827 -- ray stop 2022 -09-21 13 :22:00,515 INFO scripts.py:905 -- --block 2022 -09-21 13 :22:00,515 INFO scripts.py:907 -- This command will now block forever until terminated by a signal. 2022 -09-21 13 :22:00,515 INFO scripts.py:910 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported. Execute hello world Ray code $ kubectl exec raycluster-heterogeneous-head-9t28q -- python /opt/sample_code.py 2022 -09-21 13 :28:41,176 INFO worker.py:1224 -- Using address 127 .0.0.1:6379 set in the environment variable RAY_ADDRESS 2022 -09-21 13 :28:41,176 INFO worker.py:1333 -- Connecting to existing Ray cluster at address: 10 .244.0.144:6379... 2022 -09-21 13 :28:41,183 INFO worker.py:1515 -- Connected to Ray cluster. View the dashboard at http://10.244.0.144:8265 trying to connect to Ray! now executing some code with Ray! Ray Nodes: { '10.244.0.145' , '10.244.0.143' , '10.244.0.146' , '10.244.0.144' , '10.244.0.147' } Execution time = 4 .855740308761597 The output of hello world Ray code show 5 nodes in the Ray cluster Ray Nodes: {'10.244.0.145', '10.244.0.143', '10.244.0.146', '10.244.0.144', '10.244.0.147'} # Delete the cluster. $ kubectl delete raycluster raycluster-heterogeneous","title":"Running an example cluster"},{"location":"deploy/docker/","text":"Docker images \u00b6 Find the Docker images for various KubeRay components on Dockerhub . Stable versions \u00b6 For stable releases, use version tags (e.g. kuberay/operator:v0.3.0 ). Master commits \u00b6 The first seven characters of the git SHA specify images built from specific commits (e.g. kuberay/operator:944a042 ). Nightly images \u00b6 The nightly tag specifies images built from the most recent master (e.g. kuberay/operator:nightly ).","title":"Docker Images"},{"location":"deploy/docker/#docker-images","text":"Find the Docker images for various KubeRay components on Dockerhub .","title":"Docker images"},{"location":"deploy/docker/#stable-versions","text":"For stable releases, use version tags (e.g. kuberay/operator:v0.3.0 ).","title":"Stable versions"},{"location":"deploy/docker/#master-commits","text":"The first seven characters of the git SHA specify images built from specific commits (e.g. kuberay/operator:944a042 ).","title":"Master commits"},{"location":"deploy/docker/#nightly-images","text":"The nightly tag specifies images built from the most recent master (e.g. kuberay/operator:nightly ).","title":"Nightly images"},{"location":"deploy/helm-cluster/","text":"Ray Cluster \u00b6 Make sure ray-operator has been deployed. Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for simplifying ML compute. Helm \u00b6 $ helm version version.BuildInfo{Version:\"v3.6.2\", GitCommit:\"ee407bdf364942bcb8e8c665f82e15aa28009b71\", GitTreeState:\"dirty\", GoVersion:\"go1.16.5\"} TL;DR; \u00b6 # Because the ray-cluster chart in release 0.3.0 has some bugs, we need to clone the KubeRay repo and install the latest ray-cluster chart until release 0.4.0. cd helm-chart/ray-cluster helm install ray-cluster --namespace ray-system --create-namespace . Installing the Chart \u00b6 To install the chart with the release name my-release : # Because the ray-cluster chart in release 0.3.0 has some bugs, we need to clone the KubeRay repo and install the latest ray-cluster chart until release 0.4.0. cd helm-chart/ray-cluster helm install my-release --namespace ray-system --create-namespace . note: The chart will submit a RayCluster. Uninstalling the Chart \u00b6 To uninstall/delete the my-release deployment: helm delete my-release -n ray-system The command removes nearly all the Kubernetes components associated with the chart and deletes the release. Check Cluster status \u00b6 Get Service \u00b6 $ kubectl get svc -l ray.io/cluster = ray-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ray-cluster-head-svc ClusterIP 10.103.36.68 <none> 10001/TCP,6379/TCP,8265/TCP 9m24s Forward to dashboard \u00b6 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ray-cluster-head-sd77l 1/1 Running 0 8h 10.1.61.208 docker-desktop <none> <none> ray-cluster-worker-workergroup-czxd6 1/1 Running 0 8h 10.1.61.207 docker-desktop <none> <none> kuberay-operator-687785b964-jgfhv 1/1 Running 6 3d4h 10.1.61.196 docker-desktop <none> <none> $ kubectl port-forward ray-cluster-head-sd77l 8265 Forwarding from 127.0.0.1:8265 -> 8265 Forwarding from [::1]:8265 -> 8265","title":"Installation(Helm-cluster)"},{"location":"deploy/helm-cluster/#ray-cluster","text":"Make sure ray-operator has been deployed. Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a toolkit of libraries (Ray AIR) for simplifying ML compute.","title":"Ray Cluster"},{"location":"deploy/helm-cluster/#helm","text":"$ helm version version.BuildInfo{Version:\"v3.6.2\", GitCommit:\"ee407bdf364942bcb8e8c665f82e15aa28009b71\", GitTreeState:\"dirty\", GoVersion:\"go1.16.5\"}","title":"Helm"},{"location":"deploy/helm-cluster/#tldr","text":"# Because the ray-cluster chart in release 0.3.0 has some bugs, we need to clone the KubeRay repo and install the latest ray-cluster chart until release 0.4.0. cd helm-chart/ray-cluster helm install ray-cluster --namespace ray-system --create-namespace .","title":"TL;DR;"},{"location":"deploy/helm-cluster/#installing-the-chart","text":"To install the chart with the release name my-release : # Because the ray-cluster chart in release 0.3.0 has some bugs, we need to clone the KubeRay repo and install the latest ray-cluster chart until release 0.4.0. cd helm-chart/ray-cluster helm install my-release --namespace ray-system --create-namespace . note: The chart will submit a RayCluster.","title":"Installing the Chart"},{"location":"deploy/helm-cluster/#uninstalling-the-chart","text":"To uninstall/delete the my-release deployment: helm delete my-release -n ray-system The command removes nearly all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"deploy/helm-cluster/#check-cluster-status","text":"","title":"Check Cluster status"},{"location":"deploy/helm-cluster/#get-service","text":"$ kubectl get svc -l ray.io/cluster = ray-cluster NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ray-cluster-head-svc ClusterIP 10.103.36.68 <none> 10001/TCP,6379/TCP,8265/TCP 9m24s","title":"Get Service"},{"location":"deploy/helm-cluster/#forward-to-dashboard","text":"$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ray-cluster-head-sd77l 1/1 Running 0 8h 10.1.61.208 docker-desktop <none> <none> ray-cluster-worker-workergroup-czxd6 1/1 Running 0 8h 10.1.61.207 docker-desktop <none> <none> kuberay-operator-687785b964-jgfhv 1/1 Running 6 3d4h 10.1.61.196 docker-desktop <none> <none> $ kubectl port-forward ray-cluster-head-sd77l 8265 Forwarding from 127.0.0.1:8265 -> 8265 Forwarding from [::1]:8265 -> 8265","title":"Forward to dashboard"},{"location":"deploy/helm/","text":"KubeRay Operator \u00b6 Kuberay-operator: A simple Helm chart Run a deployment of Ray Operator. Deploy ray operator first, then deploy ray cluster. Helm \u00b6 Make sure helm version is v3+ $ helm version version.BuildInfo{Version:\"v3.6.2\", GitCommit:\"ee407bdf364942bcb8e8c665f82e15aa28009b71\", GitTreeState:\"dirty\", GoVersion:\"go1.16.5\"} Installing the Chart \u00b6 To avoid duplicate CRD definitions in this repo, we reuse CRD config in ray-operator : $ kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v0.3.0&timeout=90s\" Note that we must use kubectl create to install the CRDs. The corresponding kubectl apply command will not work. See KubeRay issue #271 . Use the following command to install the chart: $ helm install kuberay-operator --namespace ray-system --create-namespace $( curl -s https://api.github.com/repos/ray-project/kuberay/releases/tags/v0.3.0 | grep '\"browser_download_url\":' | sort | grep -om1 'https.*helm-chart-kuberay-operator.*tgz' ) List the Chart \u00b6 To list the my-release deployment: $ helm list -n ray-system Uninstalling the Chart \u00b6 To uninstall/delete the my-release deployment: $ helm delete kuberay-operator -n ray-system The command removes nearly all the Kubernetes components associated with the chart and deletes the release. Working with Argo CD \u00b6 If you are using Argo CD to manage the operator, you will encounter the issue which complains the CRDs too long. Same with this issue . The recommended solution is to split the operator into two Argo apps, such as: The first app just for installing the CRDs with Replace=true directly, snippet: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ray-operator-crds spec : project : default source : repoURL : https://github.com/ray-project/kuberay targetRevision : v0.3.0 path : helm-chart/kuberay-operator/crds destination : server : https://kubernetes.default.svc syncPolicy : syncOptions : - Replace=true ... The second app that installs the Helm chart with skipCrds=true (new feature in Argo CD 2.3.0), snippet: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ray-operator spec : source : repoURL : https://github.com/ray-project/kuberay targetRevision : v0.3.0 path : helm-chart/kuberay-operator helm : skipCrds : true destination : server : https://kubernetes.default.svc namespace : ray-operator syncPolicy : syncOptions : - CreateNamespace=true ...","title":"Installation(Helm)"},{"location":"deploy/helm/#kuberay-operator","text":"Kuberay-operator: A simple Helm chart Run a deployment of Ray Operator. Deploy ray operator first, then deploy ray cluster.","title":"KubeRay Operator"},{"location":"deploy/helm/#helm","text":"Make sure helm version is v3+ $ helm version version.BuildInfo{Version:\"v3.6.2\", GitCommit:\"ee407bdf364942bcb8e8c665f82e15aa28009b71\", GitTreeState:\"dirty\", GoVersion:\"go1.16.5\"}","title":"Helm"},{"location":"deploy/helm/#installing-the-chart","text":"To avoid duplicate CRD definitions in this repo, we reuse CRD config in ray-operator : $ kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/crd?ref=v0.3.0&timeout=90s\" Note that we must use kubectl create to install the CRDs. The corresponding kubectl apply command will not work. See KubeRay issue #271 . Use the following command to install the chart: $ helm install kuberay-operator --namespace ray-system --create-namespace $( curl -s https://api.github.com/repos/ray-project/kuberay/releases/tags/v0.3.0 | grep '\"browser_download_url\":' | sort | grep -om1 'https.*helm-chart-kuberay-operator.*tgz' )","title":"Installing the Chart"},{"location":"deploy/helm/#list-the-chart","text":"To list the my-release deployment: $ helm list -n ray-system","title":"List the Chart"},{"location":"deploy/helm/#uninstalling-the-chart","text":"To uninstall/delete the my-release deployment: $ helm delete kuberay-operator -n ray-system The command removes nearly all the Kubernetes components associated with the chart and deletes the release.","title":"Uninstalling the Chart"},{"location":"deploy/helm/#working-with-argo-cd","text":"If you are using Argo CD to manage the operator, you will encounter the issue which complains the CRDs too long. Same with this issue . The recommended solution is to split the operator into two Argo apps, such as: The first app just for installing the CRDs with Replace=true directly, snippet: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ray-operator-crds spec : project : default source : repoURL : https://github.com/ray-project/kuberay targetRevision : v0.3.0 path : helm-chart/kuberay-operator/crds destination : server : https://kubernetes.default.svc syncPolicy : syncOptions : - Replace=true ... The second app that installs the Helm chart with skipCrds=true (new feature in Argo CD 2.3.0), snippet: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ray-operator spec : source : repoURL : https://github.com/ray-project/kuberay targetRevision : v0.3.0 path : helm-chart/kuberay-operator helm : skipCrds : true destination : server : https://kubernetes.default.svc namespace : ray-operator syncPolicy : syncOptions : - CreateNamespace=true ...","title":"Working with Argo CD"},{"location":"deploy/installation/","text":"Installation \u00b6 Make sure your Kubernetes cluster and Kubectl are both at version at least 1.19. Nightly version \u00b6 export KUBERAY_VERSION=master kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=${KUBERAY_VERSION}&timeout=90s\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=${KUBERAY_VERSION}&timeout=90s\" Stable version \u00b6 kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.3.0\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=v0.3.0\" Observe that we must use kubectl create to install cluster-scoped resources. The corresponding kubectl apply command will not work. See KubeRay issue #271 . Single Namespace version \u00b6 It is possible that the user can only access one single namespace while deploying KubeRay. To deploy KubeRay in a single namespace, the user can use following commands. # Nightly version export KUBERAY_NAMESPACE=<my-awesome-namespace> # executed by cluster admin kustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace-resources\" | envsubst | kubectl create -f - # executed by user kustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace\" | envsubst | kubectl apply -f -","title":"Installation(Yaml)"},{"location":"deploy/installation/#installation","text":"Make sure your Kubernetes cluster and Kubectl are both at version at least 1.19.","title":"Installation"},{"location":"deploy/installation/#nightly-version","text":"export KUBERAY_VERSION=master kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=${KUBERAY_VERSION}&timeout=90s\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=${KUBERAY_VERSION}&timeout=90s\"","title":"Nightly version"},{"location":"deploy/installation/#stable-version","text":"kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.3.0\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=v0.3.0\" Observe that we must use kubectl create to install cluster-scoped resources. The corresponding kubectl apply command will not work. See KubeRay issue #271 .","title":"Stable version"},{"location":"deploy/installation/#single-namespace-version","text":"It is possible that the user can only access one single namespace while deploying KubeRay. To deploy KubeRay in a single namespace, the user can use following commands. # Nightly version export KUBERAY_NAMESPACE=<my-awesome-namespace> # executed by cluster admin kustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace-resources\" | envsubst | kubectl create -f - # executed by user kustomize build \"github.com/ray-project/kuberay/manifests/overlays/single-namespace\" | envsubst | kubectl apply -f -","title":"Single Namespace version"},{"location":"design/protobuf-grpc-service/","text":"Support proto Core API and RESTful backend services \u00b6 Motivation \u00b6 There're few major blockers for users to use KubeRay Operator directly. Current ray operator is only friendly to users who is familiar with Kubernetes operator pattern. For most data scientists, there's still a learning curve. Using kubectl requires sophisticated permission system. Some kubernetes clusters do not enable user level authentication. In some companies, devops use loose RBAC management and corp SSO system is not integrated with Kubernetes OIDC at all. Due to above reason, it's worth to build generic abstraction on top of RayCluster CRD. With the core api support, we can easily build backend services, cli, etc to bridge users without Kubernetes experiences to KubeRay. Goals \u00b6 The api definition should be flexible enough to support different kinds of clients (e.g. backend, cli etc). This backend service underneath should leverage generate clients to interact with existing RayCluster custom resources. New added components should be plugable to existing operator. Proposal \u00b6 Deployment topology and interactive flow \u00b6 The new gRPC service would be a individual deployment of the KubeRay control plane and user can choose to install it optionally. It will create a service and exposes endpoint to users. NAME READY STATUS RESTARTS AGE kuberay-grpc-service-c8db9dc65-d4w5r 1/1 Running 0 2d15h kuberay-operator-785476b948-fmlm7 1/1 Running 0 3d In issue #29 , RayCluster CRD clientset has been generated and gRPC service can leverage it to operate Custom Resources. A simple flow would be like this. (Thanks @akanso for providing the flow) client --> GRPC Server --> [created Custom Resources] <-- Ray Operator (reads CR and accordingly performs CRUD) API abstraction \u00b6 Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. Protoc also provides different community plugins to meet different needs. In order to better define resources at the API level, a few proto files will be defined. Technically, we can use similar data structure like RayCluster Kubernetes resource but this is probably not a good idea. Some of the Kubernetes API like tolerance and node affinity are too complicated to be converted to an API. We want to leave some flexibility to use database to store history data in the near future (for example, pagination, list options etc). We end up propsing a simple and easy API which can cover most of the daily requirements. service ClusterService { // Creates a new Cluster. rpc CreateCluster(CreateClusterRequest) returns (Cluster) { option (google.api.http) = { post: \"/apis/v1alpha2/namespaces/{namespace}/clusters\" body: \"cluster\" }; } // Finds a specific Cluster by ID. rpc GetCluster(GetClusterRequest) returns (Cluster) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\" }; } // Finds all Clusters in a given namespace. Supports pagination, and sorting on certain fields. rpc ListCluster(ListClustersRequest) returns (ListClustersResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/clusters\" }; } // Finds all Clusters in all namespaces. Supports pagination, and sorting on certain fields. rpc ListAllClusters(ListAllClustersRequest) returns (ListAllClustersResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/clusters\" }; } // Deletes an cluster without deleting the cluster's runs and jobs. To // avoid unexpected behaviors, delete an cluster's runs and jobs before // deleting the cluster. rpc DeleteCluster(DeleteClusterRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\" }; } } message CreateClusterRequest { // The cluster to be created. Cluster cluster = 1; // The namespace of the cluster to be created. string namespace = 2; } message GetClusterRequest { // The name of the cluster to be retrieved. string name = 1; // The namespace of the cluster to be retrieved. string namespace = 2; } message ListClustersRequest { // The namespace of the clusters to be retrieved. string namespace = 1; } message ListClustersResponse { // A list of clusters returned. repeated Cluster clusters = 1; } message ListAllClustersRequest {} message ListAllClustersResponse { // A list of clusters returned. repeated Cluster clusters = 1; } message DeleteClusterRequest { // The name of the cluster to be deleted. string name = 1; // The namespace of the cluster to be deleted. string namespace = 2; } message Cluster { // Required input field. Unique cluster name provided by user. string name = 1; // Required input field. Cluster's namespace provided by user string namespace = 2; // Required field. This field indicates the user who owns the cluster. string user = 3; // Optional input field. Ray cluster version string version = 4; // Optional field. enum Environment { DEV = 0; TESTING = 1; STAGING = 2; PRODUCTION = 3; } Environment environment = 5; // Required field. This field indicates ray cluster configuration ClusterSpec cluster_spec = 6; // Output. The time that the cluster created. google.protobuf.Timestamp created_at = 7; // Output. The time that the cluster deleted. google.protobuf.Timestamp deleted_at = 8; } message ClusterSpec { // The head group configuration HeadGroupSpec head_group_spec = 1; // The worker group configurations repeated WorkerGroupSpec worker_group_spec = 2; } message HeadGroupSpec { // Optional. The computeTemplate of head node group string compute_template = 1; // Optional field. This field will be used to retrieve right ray container string image = 2; // Optional. The service type (ClusterIP, NodePort, Load balancer) of the head node string service_type = 3; // Optional. The ray start parames of head node group map<string, string> ray_start_params = 4; } message WorkerGroupSpec { // Required. Group name of the current worker group string group_name = 1; // Optional. The computeTemplate of head node group string compute_template = 2; // Optional field. This field will be used to retrieve right ray container string image = 3; // Required. Desired replicas of the worker group int32 replicas = 4; // Optional. Min replicas of the worker group int32 min_replicas = 5; // Optional. Max replicas of the worker group int32 max_replicas = 6; // Optional. The ray start parames of worker node group map<string, string> ray_start_params = 7; } service ComputeTemplateService { // Creates a new compute template. rpc CreateComputeTemplate(CreateComputeTemplateRequest) returns (ComputeTemplate) { option (google.api.http) = { post: \"/apis/v1alpha2/compute_templates\" body: \"compute_template\" }; } // Finds a specific compute template by its name and namespace. rpc GetComputeTemplate(GetComputeTemplateRequest) returns (ComputeTemplate) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates/{name}\" }; } // Finds all compute templates in a given namespace. Supports pagination, and sorting on certain fields. rpc ListComputeTemplates(ListComputeTemplatesRequest) returns (ListComputeTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates\" }; } // Finds all compute templates in all namespaces. Supports pagination, and sorting on certain fields. rpc ListAllComputeTemplates(ListAllComputeTemplatesRequest) returns (ListAllComputeTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/compute_templates\" }; } // Deletes a compute template by its name and namespace rpc DeleteComputeTemplate(DeleteComputeTemplateRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates/{name}\" }; } } message CreateComputeTemplateRequest { // The compute template to be created. ComputeTemplate compute_template = 1; // The namespace of the compute template to be created string namespace = 2; } message GetComputeTemplateRequest { // The name of the ComputeTemplate to be retrieved. string name = 1; // The namespace of the compute template to be retrieved. string namespace = 2; } message ListComputeTemplatesRequest { // The namespace of the compute templates to be retrieved. string namespace = 1; // TODO: support paganation later } message ListComputeTemplatesResponse { repeated ComputeTemplate compute_templates = 1; } message ListAllComputeTemplatesRequest { // TODO: support paganation later } message ListAllComputeTemplatesResponse { repeated ComputeTemplate compute_templates = 1; } message DeleteComputeTemplateRequest { // The name of the compute template to be deleted. string name = 1; // The namespace of the compute template to be deleted. string namespace = 2; } // ComputeTemplate can be reused by any compute units like worker group, workspace, image build job, etc message ComputeTemplate { // The name of the compute template string name = 1; // The namespace of the compute template string namespace = 2; // Number of cpus uint32 cpu = 3; // Number of memory uint32 memory = 4; // Number of gpus uint32 gpu = 5; // The detail gpu accelerator type string gpu_accelerator = 6; } service ImageTemplateService { // Creates a new ImageTemplate. rpc CreateImageTemplate(CreateImageTemplateRequest) returns (ImageTemplate) { option (google.api.http) = { post: \"/apis/v1alpha2/image_templates\" body: \"image_template\" }; } // Finds a specific ImageTemplate by ID. rpc GetImageTemplate(GetImageTemplateRequest) returns (ImageTemplate) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/image_templates/{name}\" }; } // Finds all ImageTemplates. Supports pagination, and sorting on certain fields. rpc ListImageTemplates(ListImageTemplatesRequest) returns (ListImageTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/image_templates\" }; } // Deletes an ImageTemplate. rpc DeleteImageTemplate(DeleteImageTemplateRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/image_templates/{name}\" }; } } message CreateImageTemplateRequest { // The image template to be created. ImageTemplate image_template = 1; // The namespace of the image template to be created. string namespace = 2; } message GetImageTemplateRequest { // The name of the image template to be retrieved. string name = 1; // The namespace of the image template to be retrieved. string namespace = 2; } message ListImageTemplatesRequest { // The namespace of the image templates to be retrieved. string namespace = 1; // TODO: support pagingation later } message ListImageTemplatesResponse { // A list of Compute returned. repeated ImageTemplate image_templates = 1; } message ListAllImageTemplatesRequest { // TODO: support pagingation later } message ListAllImageTemplatesResponse { // A list of Compute returned. repeated ImageTemplate image_templates = 1; } message DeleteImageTemplateRequest { // The name of the image template to be deleted. string name = 1; // The namespace of the image template to be deleted. string namespace = 2; } // ImageTemplate can be used by worker group and workspce. // They can be distinguish by different entrypoints message ImageTemplate { // The ID of the image template string name = 1; // The namespace of the image template string namespace = 2; // The base container image to be used for image building string base_image = 3; // The pip packages to install repeated string pip_packages = 4; // The conda packages to install repeated string conda_packages = 5; // The system packages to install repeated string system_packages = 6; // The environment variables to set map<string, string> environment_variables = 7; // The post install commands to execute string custom_commands = 8; // Output. The result image generated string image = 9; } message Status { string error = 1; int32 code = 2; repeated google.protobuf.Any details = 3; } Support multiple clients \u00b6 Since we may have different clients to interactive with our services, we will generate gateway RESTful APIs and OpenAPI Spec at the same time. .proto define core api, grpc and gateway services. go_client and swagger can be generated easily for further usage. gRPC services \u00b6 The GRPC protocol provides an extremely efficient way of cross-service communication for distributed applications. The public toolkit includes instruments to generate client and server code-bases for many languages allowing the developer to use the most optimal language for the task. The service will implement gPRC server as following graph shows. A ResourceManager will be used to abstract the implementation of CRUD operators. ClientManager manages kubernetes clients which can operate Kubernetes native resource and custom resources like RayCluster. RayClusterClient comes from code generator of CRD. issue#29 Implementation History \u00b6 2021-11-25: inital proposal accepted. Note: we should update doc when there's a large update.","title":"Core API and Backend Service"},{"location":"design/protobuf-grpc-service/#support-proto-core-api-and-restful-backend-services","text":"","title":"Support proto Core API and RESTful backend services"},{"location":"design/protobuf-grpc-service/#motivation","text":"There're few major blockers for users to use KubeRay Operator directly. Current ray operator is only friendly to users who is familiar with Kubernetes operator pattern. For most data scientists, there's still a learning curve. Using kubectl requires sophisticated permission system. Some kubernetes clusters do not enable user level authentication. In some companies, devops use loose RBAC management and corp SSO system is not integrated with Kubernetes OIDC at all. Due to above reason, it's worth to build generic abstraction on top of RayCluster CRD. With the core api support, we can easily build backend services, cli, etc to bridge users without Kubernetes experiences to KubeRay.","title":"Motivation"},{"location":"design/protobuf-grpc-service/#goals","text":"The api definition should be flexible enough to support different kinds of clients (e.g. backend, cli etc). This backend service underneath should leverage generate clients to interact with existing RayCluster custom resources. New added components should be plugable to existing operator.","title":"Goals"},{"location":"design/protobuf-grpc-service/#proposal","text":"","title":"Proposal"},{"location":"design/protobuf-grpc-service/#deployment-topology-and-interactive-flow","text":"The new gRPC service would be a individual deployment of the KubeRay control plane and user can choose to install it optionally. It will create a service and exposes endpoint to users. NAME READY STATUS RESTARTS AGE kuberay-grpc-service-c8db9dc65-d4w5r 1/1 Running 0 2d15h kuberay-operator-785476b948-fmlm7 1/1 Running 0 3d In issue #29 , RayCluster CRD clientset has been generated and gRPC service can leverage it to operate Custom Resources. A simple flow would be like this. (Thanks @akanso for providing the flow) client --> GRPC Server --> [created Custom Resources] <-- Ray Operator (reads CR and accordingly performs CRUD)","title":"Deployment topology and interactive flow"},{"location":"design/protobuf-grpc-service/#api-abstraction","text":"Protocol Buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data. Protoc also provides different community plugins to meet different needs. In order to better define resources at the API level, a few proto files will be defined. Technically, we can use similar data structure like RayCluster Kubernetes resource but this is probably not a good idea. Some of the Kubernetes API like tolerance and node affinity are too complicated to be converted to an API. We want to leave some flexibility to use database to store history data in the near future (for example, pagination, list options etc). We end up propsing a simple and easy API which can cover most of the daily requirements. service ClusterService { // Creates a new Cluster. rpc CreateCluster(CreateClusterRequest) returns (Cluster) { option (google.api.http) = { post: \"/apis/v1alpha2/namespaces/{namespace}/clusters\" body: \"cluster\" }; } // Finds a specific Cluster by ID. rpc GetCluster(GetClusterRequest) returns (Cluster) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\" }; } // Finds all Clusters in a given namespace. Supports pagination, and sorting on certain fields. rpc ListCluster(ListClustersRequest) returns (ListClustersResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/clusters\" }; } // Finds all Clusters in all namespaces. Supports pagination, and sorting on certain fields. rpc ListAllClusters(ListAllClustersRequest) returns (ListAllClustersResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/clusters\" }; } // Deletes an cluster without deleting the cluster's runs and jobs. To // avoid unexpected behaviors, delete an cluster's runs and jobs before // deleting the cluster. rpc DeleteCluster(DeleteClusterRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/clusters/{name}\" }; } } message CreateClusterRequest { // The cluster to be created. Cluster cluster = 1; // The namespace of the cluster to be created. string namespace = 2; } message GetClusterRequest { // The name of the cluster to be retrieved. string name = 1; // The namespace of the cluster to be retrieved. string namespace = 2; } message ListClustersRequest { // The namespace of the clusters to be retrieved. string namespace = 1; } message ListClustersResponse { // A list of clusters returned. repeated Cluster clusters = 1; } message ListAllClustersRequest {} message ListAllClustersResponse { // A list of clusters returned. repeated Cluster clusters = 1; } message DeleteClusterRequest { // The name of the cluster to be deleted. string name = 1; // The namespace of the cluster to be deleted. string namespace = 2; } message Cluster { // Required input field. Unique cluster name provided by user. string name = 1; // Required input field. Cluster's namespace provided by user string namespace = 2; // Required field. This field indicates the user who owns the cluster. string user = 3; // Optional input field. Ray cluster version string version = 4; // Optional field. enum Environment { DEV = 0; TESTING = 1; STAGING = 2; PRODUCTION = 3; } Environment environment = 5; // Required field. This field indicates ray cluster configuration ClusterSpec cluster_spec = 6; // Output. The time that the cluster created. google.protobuf.Timestamp created_at = 7; // Output. The time that the cluster deleted. google.protobuf.Timestamp deleted_at = 8; } message ClusterSpec { // The head group configuration HeadGroupSpec head_group_spec = 1; // The worker group configurations repeated WorkerGroupSpec worker_group_spec = 2; } message HeadGroupSpec { // Optional. The computeTemplate of head node group string compute_template = 1; // Optional field. This field will be used to retrieve right ray container string image = 2; // Optional. The service type (ClusterIP, NodePort, Load balancer) of the head node string service_type = 3; // Optional. The ray start parames of head node group map<string, string> ray_start_params = 4; } message WorkerGroupSpec { // Required. Group name of the current worker group string group_name = 1; // Optional. The computeTemplate of head node group string compute_template = 2; // Optional field. This field will be used to retrieve right ray container string image = 3; // Required. Desired replicas of the worker group int32 replicas = 4; // Optional. Min replicas of the worker group int32 min_replicas = 5; // Optional. Max replicas of the worker group int32 max_replicas = 6; // Optional. The ray start parames of worker node group map<string, string> ray_start_params = 7; } service ComputeTemplateService { // Creates a new compute template. rpc CreateComputeTemplate(CreateComputeTemplateRequest) returns (ComputeTemplate) { option (google.api.http) = { post: \"/apis/v1alpha2/compute_templates\" body: \"compute_template\" }; } // Finds a specific compute template by its name and namespace. rpc GetComputeTemplate(GetComputeTemplateRequest) returns (ComputeTemplate) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates/{name}\" }; } // Finds all compute templates in a given namespace. Supports pagination, and sorting on certain fields. rpc ListComputeTemplates(ListComputeTemplatesRequest) returns (ListComputeTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates\" }; } // Finds all compute templates in all namespaces. Supports pagination, and sorting on certain fields. rpc ListAllComputeTemplates(ListAllComputeTemplatesRequest) returns (ListAllComputeTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/compute_templates\" }; } // Deletes a compute template by its name and namespace rpc DeleteComputeTemplate(DeleteComputeTemplateRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/compute_templates/{name}\" }; } } message CreateComputeTemplateRequest { // The compute template to be created. ComputeTemplate compute_template = 1; // The namespace of the compute template to be created string namespace = 2; } message GetComputeTemplateRequest { // The name of the ComputeTemplate to be retrieved. string name = 1; // The namespace of the compute template to be retrieved. string namespace = 2; } message ListComputeTemplatesRequest { // The namespace of the compute templates to be retrieved. string namespace = 1; // TODO: support paganation later } message ListComputeTemplatesResponse { repeated ComputeTemplate compute_templates = 1; } message ListAllComputeTemplatesRequest { // TODO: support paganation later } message ListAllComputeTemplatesResponse { repeated ComputeTemplate compute_templates = 1; } message DeleteComputeTemplateRequest { // The name of the compute template to be deleted. string name = 1; // The namespace of the compute template to be deleted. string namespace = 2; } // ComputeTemplate can be reused by any compute units like worker group, workspace, image build job, etc message ComputeTemplate { // The name of the compute template string name = 1; // The namespace of the compute template string namespace = 2; // Number of cpus uint32 cpu = 3; // Number of memory uint32 memory = 4; // Number of gpus uint32 gpu = 5; // The detail gpu accelerator type string gpu_accelerator = 6; } service ImageTemplateService { // Creates a new ImageTemplate. rpc CreateImageTemplate(CreateImageTemplateRequest) returns (ImageTemplate) { option (google.api.http) = { post: \"/apis/v1alpha2/image_templates\" body: \"image_template\" }; } // Finds a specific ImageTemplate by ID. rpc GetImageTemplate(GetImageTemplateRequest) returns (ImageTemplate) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/image_templates/{name}\" }; } // Finds all ImageTemplates. Supports pagination, and sorting on certain fields. rpc ListImageTemplates(ListImageTemplatesRequest) returns (ListImageTemplatesResponse) { option (google.api.http) = { get: \"/apis/v1alpha2/namespaces/{namespace}/image_templates\" }; } // Deletes an ImageTemplate. rpc DeleteImageTemplate(DeleteImageTemplateRequest) returns (google.protobuf.Empty) { option (google.api.http) = { delete: \"/apis/v1alpha2/namespaces/{namespace}/image_templates/{name}\" }; } } message CreateImageTemplateRequest { // The image template to be created. ImageTemplate image_template = 1; // The namespace of the image template to be created. string namespace = 2; } message GetImageTemplateRequest { // The name of the image template to be retrieved. string name = 1; // The namespace of the image template to be retrieved. string namespace = 2; } message ListImageTemplatesRequest { // The namespace of the image templates to be retrieved. string namespace = 1; // TODO: support pagingation later } message ListImageTemplatesResponse { // A list of Compute returned. repeated ImageTemplate image_templates = 1; } message ListAllImageTemplatesRequest { // TODO: support pagingation later } message ListAllImageTemplatesResponse { // A list of Compute returned. repeated ImageTemplate image_templates = 1; } message DeleteImageTemplateRequest { // The name of the image template to be deleted. string name = 1; // The namespace of the image template to be deleted. string namespace = 2; } // ImageTemplate can be used by worker group and workspce. // They can be distinguish by different entrypoints message ImageTemplate { // The ID of the image template string name = 1; // The namespace of the image template string namespace = 2; // The base container image to be used for image building string base_image = 3; // The pip packages to install repeated string pip_packages = 4; // The conda packages to install repeated string conda_packages = 5; // The system packages to install repeated string system_packages = 6; // The environment variables to set map<string, string> environment_variables = 7; // The post install commands to execute string custom_commands = 8; // Output. The result image generated string image = 9; } message Status { string error = 1; int32 code = 2; repeated google.protobuf.Any details = 3; }","title":"API abstraction"},{"location":"design/protobuf-grpc-service/#support-multiple-clients","text":"Since we may have different clients to interactive with our services, we will generate gateway RESTful APIs and OpenAPI Spec at the same time. .proto define core api, grpc and gateway services. go_client and swagger can be generated easily for further usage.","title":"Support multiple clients"},{"location":"design/protobuf-grpc-service/#grpc-services","text":"The GRPC protocol provides an extremely efficient way of cross-service communication for distributed applications. The public toolkit includes instruments to generate client and server code-bases for many languages allowing the developer to use the most optimal language for the task. The service will implement gPRC server as following graph shows. A ResourceManager will be used to abstract the implementation of CRUD operators. ClientManager manages kubernetes clients which can operate Kubernetes native resource and custom resources like RayCluster. RayClusterClient comes from code generator of CRD. issue#29","title":"gRPC services"},{"location":"design/protobuf-grpc-service/#implementation-history","text":"2021-11-25: inital proposal accepted. Note: we should update doc when there's a large update.","title":"Implementation History"},{"location":"development/development/","text":"KubeRay Development Guidance \u00b6 Download this repo locally mkdir -p $GOPATH/src/github.com/ray-project cd $GOPATH/src/github.com/ray-project git clone https://github.com/ray-project/kuberay.git Develop proto and OpenAPI \u00b6 Generate go clients and swagger file make generate Develop KubeRay Operator \u00b6 cd ray-operator # Build codes make build # Run test make test # Build container image make docker-build Develop KubeRay APIServer \u00b6 cd apiserver # Build code go build cmd/main.go Develop KubeRay CLI \u00b6 cd cli go build -o kuberay -a main.go ./kuberay help Deploy Docs locally \u00b6 We don't need to configure mkdocs environment. To check the static website locally, run the command docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material and access http://0.0.0.0:8000/kuberay/ in your web browser.","title":"Development"},{"location":"development/development/#kuberay-development-guidance","text":"Download this repo locally mkdir -p $GOPATH/src/github.com/ray-project cd $GOPATH/src/github.com/ray-project git clone https://github.com/ray-project/kuberay.git","title":"KubeRay Development Guidance"},{"location":"development/development/#develop-proto-and-openapi","text":"Generate go clients and swagger file make generate","title":"Develop proto and OpenAPI"},{"location":"development/development/#develop-kuberay-operator","text":"cd ray-operator # Build codes make build # Run test make test # Build container image make docker-build","title":"Develop KubeRay Operator"},{"location":"development/development/#develop-kuberay-apiserver","text":"cd apiserver # Build code go build cmd/main.go","title":"Develop KubeRay APIServer"},{"location":"development/development/#develop-kuberay-cli","text":"cd cli go build -o kuberay -a main.go ./kuberay help","title":"Develop KubeRay CLI"},{"location":"development/development/#deploy-docs-locally","text":"We don't need to configure mkdocs environment. To check the static website locally, run the command docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material and access http://0.0.0.0:8000/kuberay/ in your web browser.","title":"Deploy Docs locally"},{"location":"development/release/","text":"KubeRay Release Process \u00b6 Prerequisite \u00b6 Github Write permissions to cut a release tag/branch. Dockerhub Write permissions to push images with pinned tag. Release Process \u00b6 Make sure the last commit you want to release past Go-build-and-test workflow. Check out that commit (in this example, we'll use 6214e560 ). Depends on what version you want to release, Major or Minor version - Use the GitHub UI to cut a release branch and name the release branch v{MAJOR}.${MINOR}-branch Patch version - You don't need to cut release branch on patch version. Tag the image version from kuberay/operator:6214e560 to kuberay/operator:v0.2.0 and push to dockerhub. docker tag kuberay/operator:6214e560 kuberay/operator:v0.2.0 docker push kuberay/operator:v0.2.0 docker tag kuberay/apiserver:6214e560 kuberay/apiserver:v0.2.0 docker push kuberay/apiserver:v0.2.0 Build CLI with multi arch support and they will be uploaded as release artifacts in later step. Create a new PR against the release branch to change container image in manifest to point to that commit hash. images: - name: kuberay/operator newName: kuberay/operator newTag: v0.2.0 ... note: post submit job will always build a new image using the PULL_BASE_HASH as image tag. Create a tag and push tag to upstream. git tag v0.2.0 git push upstream v0.2.0 Run following code and fetch online git commits from last release (v0.1.0) to current release (v0.2.0). git log v0.1.0..v0.2.0 --oneline Generate release notes and update Github release. See v0.1.0 example here . Please also upload CLI binaries. Send a PR to update CHANGELOG.md","title":"Release"},{"location":"development/release/#kuberay-release-process","text":"","title":"KubeRay Release Process"},{"location":"development/release/#prerequisite","text":"Github Write permissions to cut a release tag/branch. Dockerhub Write permissions to push images with pinned tag.","title":"Prerequisite"},{"location":"development/release/#release-process","text":"Make sure the last commit you want to release past Go-build-and-test workflow. Check out that commit (in this example, we'll use 6214e560 ). Depends on what version you want to release, Major or Minor version - Use the GitHub UI to cut a release branch and name the release branch v{MAJOR}.${MINOR}-branch Patch version - You don't need to cut release branch on patch version. Tag the image version from kuberay/operator:6214e560 to kuberay/operator:v0.2.0 and push to dockerhub. docker tag kuberay/operator:6214e560 kuberay/operator:v0.2.0 docker push kuberay/operator:v0.2.0 docker tag kuberay/apiserver:6214e560 kuberay/apiserver:v0.2.0 docker push kuberay/apiserver:v0.2.0 Build CLI with multi arch support and they will be uploaded as release artifacts in later step. Create a new PR against the release branch to change container image in manifest to point to that commit hash. images: - name: kuberay/operator newName: kuberay/operator newTag: v0.2.0 ... note: post submit job will always build a new image using the PULL_BASE_HASH as image tag. Create a tag and push tag to upstream. git tag v0.2.0 git push upstream v0.2.0 Run following code and fetch online git commits from last release (v0.1.0) to current release (v0.2.0). git log v0.1.0..v0.2.0 --oneline Generate release notes and update Github release. See v0.1.0 example here . Please also upload CLI binaries. Send a PR to update CHANGELOG.md","title":"Release Process"},{"location":"guidance/autoscaler/","text":"Autoscaler (beta) \u00b6 Prerequisite \u00b6 Ray Autoscaler integration is beta with KubeRay 0.3.0 and Ray 2.0.0. While autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases. Start by deploying the latest stable version of the KubeRay operator: kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.3.0-rc.1&timeout=90s\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=v0.3.0-rc.1&timeout=90s\" Deploy a cluster with autoscaling enabled \u00b6 Next, to deploy a sample autoscaling Ray cluster, run kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/release-0.3/ray-operator/config/samples/ray-cluster.autoscaler.yaml See the above config file for details on autoscaling configuration. The output of kubectl get pods should indicate the presence of two containers, the Ray container and the autoscaler container. $ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-autoscaler-head-mgwwk 2/2 Running 0 4m41s Check the autoscaler container's logs to confirm that the autoscaler is healthy. Here's an example of logs from a healthy autoscaler. kubectl logs -f raycluster-autoscaler-head-mgwwk autoscaler 2022-03-10 07:51:22,616 INFO monitor.py:226 -- Starting autoscaler metrics server on port 44217 2022-03-10 07:51:22,621 INFO monitor.py:243 -- Monitor: Started 2022-03-10 07:51:22,824 INFO node_provider.py:143 -- Creating KuberayNodeProvider. 2022-03-10 07:51:22,825 INFO autoscaler.py:282 -- StandardAutoscaler: {'provider': {'type': 'kuberay', 'namespace': 'default', 'disable_node_updaters': True, 'disable_launch_config_check': True}, 'cluster_name': 'raycluster-autoscaler', 'head_node_type': 'head-group', 'available_node_types': {'head-group': {'min_workers': 0, 'max_workers': 0, 'node_config': {}, 'resources': {'CPU': 1}}, 'small-group': {'min_workers': 1, 'max_workers': 300, 'node_config': {}, 'resources': {'CPU': 1}}}, 'max_workers': 300, 'idle_timeout_minutes': 5, 'upscaling_speed': 1, 'file_mounts': {}, 'cluster_synced_files': [], 'file_mounts_sync_continuously': False, 'initialization_commands': [], 'setup_commands': [], 'head_setup_commands': [], 'worker_setup_commands': [], 'head_start_ray_commands': [], 'worker_start_ray_commands': [], 'auth': {}, 'head_node': {}, 'worker_nodes': {}} 2022-03-10 07:51:23,027 INFO autoscaler.py:327 -- ======== Autoscaler status: 2022-03-10 07:51:23.027271 ======== Node status --------------------------------------------------------------- Healthy: 1 head-group Pending: (no pending nodes) Recent failures: (no failures) Resources --------------------------------------------------------------- Usage: 0.0/1.0 CPU 0.00/0.931 GiB memory 0.00/0.200 GiB object_store_memory Demands: (no resource demands) Notes \u00b6 To enable autoscaling, set your RayCluster CR's spec.enableInTreeAutoscaling field to true. The operator will then automatically inject a preconfigured autoscaler container to the head pod. The service account, role, and role binding needed by the autoscaler will be created by the operator out-of-box. The operator will also configure an empty-dir logging volume for the Ray head pod. The volume will be mounted into the Ray and autoscaler containers; this is necessary to support the event logging introduced in Ray PR #13434 . spec: enableInTreeAutoscaling: true If your RayCluster CR's spec.rayVersion field is at least 2.0.0 , the autoscaler container will use the same image as the Ray container. For Ray versions older than 2.0.0, the image rayproject/ray:2.0.0 will be used to run the autoscaler. Autoscaling functionality is supported only with Ray versions at least as new as 1.11.0. Autoscaler support is beta as of Ray 2.0.0 and KubeRay 0.3.0; while autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases. Test autoscaling \u00b6 Let's now try out the autoscaler. We can run the following command to get a Python interpreter in the head pod: kubectl exec `kubectl get pods -o custom-columns=POD:metadata.name | grep raycluster-autoscaler-head` -it -c ray-head -- python In the Python interpreter, run the following snippet to scale up the cluster: import ray.autoscaler.sdk ray.init(\"auto\") ray.autoscaler.sdk.request_resources(num_cpus=4) You should then see two extra Ray nodes (pods) scale up to satisfy the 4 CPU demand.","title":"Autoscaling"},{"location":"guidance/autoscaler/#autoscaler-beta","text":"","title":"Autoscaler (beta)"},{"location":"guidance/autoscaler/#prerequisite","text":"Ray Autoscaler integration is beta with KubeRay 0.3.0 and Ray 2.0.0. While autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases. Start by deploying the latest stable version of the KubeRay operator: kubectl create -k \"github.com/ray-project/kuberay/manifests/cluster-scope-resources?ref=v0.3.0-rc.1&timeout=90s\" kubectl apply -k \"github.com/ray-project/kuberay/manifests/base?ref=v0.3.0-rc.1&timeout=90s\"","title":"Prerequisite"},{"location":"guidance/autoscaler/#deploy-a-cluster-with-autoscaling-enabled","text":"Next, to deploy a sample autoscaling Ray cluster, run kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/release-0.3/ray-operator/config/samples/ray-cluster.autoscaler.yaml See the above config file for details on autoscaling configuration. The output of kubectl get pods should indicate the presence of two containers, the Ray container and the autoscaler container. $ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-autoscaler-head-mgwwk 2/2 Running 0 4m41s Check the autoscaler container's logs to confirm that the autoscaler is healthy. Here's an example of logs from a healthy autoscaler. kubectl logs -f raycluster-autoscaler-head-mgwwk autoscaler 2022-03-10 07:51:22,616 INFO monitor.py:226 -- Starting autoscaler metrics server on port 44217 2022-03-10 07:51:22,621 INFO monitor.py:243 -- Monitor: Started 2022-03-10 07:51:22,824 INFO node_provider.py:143 -- Creating KuberayNodeProvider. 2022-03-10 07:51:22,825 INFO autoscaler.py:282 -- StandardAutoscaler: {'provider': {'type': 'kuberay', 'namespace': 'default', 'disable_node_updaters': True, 'disable_launch_config_check': True}, 'cluster_name': 'raycluster-autoscaler', 'head_node_type': 'head-group', 'available_node_types': {'head-group': {'min_workers': 0, 'max_workers': 0, 'node_config': {}, 'resources': {'CPU': 1}}, 'small-group': {'min_workers': 1, 'max_workers': 300, 'node_config': {}, 'resources': {'CPU': 1}}}, 'max_workers': 300, 'idle_timeout_minutes': 5, 'upscaling_speed': 1, 'file_mounts': {}, 'cluster_synced_files': [], 'file_mounts_sync_continuously': False, 'initialization_commands': [], 'setup_commands': [], 'head_setup_commands': [], 'worker_setup_commands': [], 'head_start_ray_commands': [], 'worker_start_ray_commands': [], 'auth': {}, 'head_node': {}, 'worker_nodes': {}} 2022-03-10 07:51:23,027 INFO autoscaler.py:327 -- ======== Autoscaler status: 2022-03-10 07:51:23.027271 ======== Node status --------------------------------------------------------------- Healthy: 1 head-group Pending: (no pending nodes) Recent failures: (no failures) Resources --------------------------------------------------------------- Usage: 0.0/1.0 CPU 0.00/0.931 GiB memory 0.00/0.200 GiB object_store_memory Demands: (no resource demands)","title":"Deploy a cluster with autoscaling enabled"},{"location":"guidance/autoscaler/#notes","text":"To enable autoscaling, set your RayCluster CR's spec.enableInTreeAutoscaling field to true. The operator will then automatically inject a preconfigured autoscaler container to the head pod. The service account, role, and role binding needed by the autoscaler will be created by the operator out-of-box. The operator will also configure an empty-dir logging volume for the Ray head pod. The volume will be mounted into the Ray and autoscaler containers; this is necessary to support the event logging introduced in Ray PR #13434 . spec: enableInTreeAutoscaling: true If your RayCluster CR's spec.rayVersion field is at least 2.0.0 , the autoscaler container will use the same image as the Ray container. For Ray versions older than 2.0.0, the image rayproject/ray:2.0.0 will be used to run the autoscaler. Autoscaling functionality is supported only with Ray versions at least as new as 1.11.0. Autoscaler support is beta as of Ray 2.0.0 and KubeRay 0.3.0; while autoscaling functionality is stable, the details of autoscaler behavior and configuration may change in future releases.","title":"Notes"},{"location":"guidance/autoscaler/#test-autoscaling","text":"Let's now try out the autoscaler. We can run the following command to get a Python interpreter in the head pod: kubectl exec `kubectl get pods -o custom-columns=POD:metadata.name | grep raycluster-autoscaler-head` -it -c ray-head -- python In the Python interpreter, run the following snippet to scale up the cluster: import ray.autoscaler.sdk ray.init(\"auto\") ray.autoscaler.sdk.request_resources(num_cpus=4) You should then see two extra Ray nodes (pods) scale up to satisfy the 4 CPU demand.","title":"Test autoscaling"},{"location":"guidance/gcs-ft/","text":"Ray GCS Fault Tolerance(GCS FT) (Experimental) \u00b6 Note: This feature is still experimental, there are a few limitations and stabilization will be done in future release from both Ray and KubeRay side. Ray GCS FT enables GCS server to use external storage backend. As a result, Ray clusters can tolerant GCS failures and recover from failures without affecting important services such as detached Actors & RayServe deployments. Prerequisite \u00b6 Ray 2.0 is required. You need to support external Redis server for Ray. (Redis HA cluster is highly recommended.) Enable Ray GCS FT \u00b6 To enable Ray GCS FT in your newly KubeRay-managed Ray cluster, you need to enable it by adding an annotation to the RayCluster YAML file. ... kind : RayCluster metadata : annotations : ray.io/ft-enabled : \"true\" # <- add this annotation enable GCS FT ray.io/external-storage-namespace : \"my-raycluster-storage-namespace\" # <- optional, to specify the external storage namespace ... An example can be found at ray-cluster.external-redis.yaml When annotation ray.io/ft-enabled is added with a true value, KubeRay will enable Ray GCS FT feature. This feature contains several components: Newly created Ray cluster has Readiness Probe and Liveness Probe added to all the head/worker nodes. KubeRay Operator controller watches for Event object changes which can notify in case of readiness probe failures and mark them as Unhealthy . KubeRay Operator controller kills and recreate any Unhealthy Ray head/worker node. Implementation Details \u00b6 Readiness Probe vs Liveness Probe \u00b6 These are the two types of probes we used in Ray GCS FT. The readiness probe is used to notify KubeRay in case of failures in the corresponding Ray cluster. KubeRay can try its best to recover the Ray cluster. If KubeRay cannot recover the failed head/worker node, the liveness probe gets in, delete the old pod and create a new pod. By default, the liveness probe gets involved later than the readiness probe. The liveness probe is our last resort to recover the Ray cluster. However, in our current implementation, for the readiness probe failures, we also kill & recreate the corresponding pod that runs head/worker node. Currently, the readiness probe and the liveness probe are using the same command to do the work. In the future, we may run different commands for the readiness probe and the liveness probe. On Ray head node, we access a local Ray dashboard http endpoint and a Raylet http endpoint to make sure this head node is in healthy state. Since Ray dashboard does not reside Ray worker node, we only check the local Raylet http endpoint to make sure the worker node is healthy. Ray GCS FT Annotation \u00b6 Our Ray GCS FT feature checks if an annotation called ray.io/ft-enabled is set to true in RayCluster YAML file. If so, KubeRay will also add such annotation to the pod whenever the head/worker node is created. Use External Redis Cluster \u00b6 To use external Redis cluster as the backend storage(required by Ray GCS FT), you need to add RAY_REDIS_ADDRESS environment variable to the head node template. Also, you can specify a storage namespace for your Ray cluster by using an annotation ray.io/external-storage-namespace An example can be found at ray-cluster.external-redis.yaml To use SSL/TLS in the connection, you add rediss:// as the prefix of the redis address instead of the redis:// prefix. This feature is only available in Ray 2.2 and above. You can also specify additional environment variables in the head pod to customize the SSL configuration: RAY_REDIS_CA_CERT The location of the CA certificate (optional) RAY_REDIS_CA_PATH Path of trusted certificates (optional) RAY_REDIS_CLIENT_CERT File name of client certificate file (optional) RAY_REDIS_CLIENT_KEY File name of client private key (optional) RAY_REDIS_SERVER_NAME Server name to request (SNI) (optional) KubeRay Operator Controller \u00b6 KubeRay Operator controller watches for new Event reconcile call. If this Event object is to notify the failed readiness probe, controller checks if this pod has ray.io/ft-enabled set to true . If this pod has this annotation set to true, that means this pod belongs to a Ray cluster that has Ray GCS FT enabled. After this, the controller will try to recover the failed pod. If controller cannot recover it, an annotation named ray.io/health-state with a value Unhealthy is added to this pod. In every KubeRay Operator controller reconcile loop, it monitors any pod in Ray cluster that has Unhealthy value in annotation ray.io/health-state . If any pod is found, this pod is deleted and gets recreated. External Storage Namespace \u00b6 External storage namespaces can be used to share a single storage backend among multiple Ray clusters. By default, ray.io/external-storage-namespace uses the RayCluster UID as its value when GCS FT is enabled. Or if the user wants to use customized external storage namespace, the user can add ray.io/external-storage-namespace annotation to RayCluster yaml file. Whenever ray.io/external-storage-namespace annotation is set, the head/worker node will have RAY_external_storage_namespace environment variable set which Ray can pick up later. Known issues and limitations \u00b6 For now, Ray head/worker node that fails the readiness probe recovers itself by restarting itself. More fine-grained control and recovery mechanisms are expected in the future. Test Ray GCS FT \u00b6 Currently, two tests are responsible for ensuring Ray GCS FT is working correctly. Detached actor test RayServe test In detached actor test, a detached actor is created at first. Then, the head node is killed. KubeRay brings back another head node replacement pod. However, the detached actor is still expected to be available. (Note: the client that creates the detached actor does not exist and will retry in case of Ray cluster returns failure) In RayServe test, a simple RayServe app is deployed on the Ray cluster. In case of GCS server crash, the RayServe app continues to be accessible after the head node recovery.","title":"Ray GCS FT"},{"location":"guidance/gcs-ft/#ray-gcs-fault-tolerancegcs-ft-experimental","text":"Note: This feature is still experimental, there are a few limitations and stabilization will be done in future release from both Ray and KubeRay side. Ray GCS FT enables GCS server to use external storage backend. As a result, Ray clusters can tolerant GCS failures and recover from failures without affecting important services such as detached Actors & RayServe deployments.","title":"Ray GCS Fault Tolerance(GCS FT) (Experimental)"},{"location":"guidance/gcs-ft/#prerequisite","text":"Ray 2.0 is required. You need to support external Redis server for Ray. (Redis HA cluster is highly recommended.)","title":"Prerequisite"},{"location":"guidance/gcs-ft/#enable-ray-gcs-ft","text":"To enable Ray GCS FT in your newly KubeRay-managed Ray cluster, you need to enable it by adding an annotation to the RayCluster YAML file. ... kind : RayCluster metadata : annotations : ray.io/ft-enabled : \"true\" # <- add this annotation enable GCS FT ray.io/external-storage-namespace : \"my-raycluster-storage-namespace\" # <- optional, to specify the external storage namespace ... An example can be found at ray-cluster.external-redis.yaml When annotation ray.io/ft-enabled is added with a true value, KubeRay will enable Ray GCS FT feature. This feature contains several components: Newly created Ray cluster has Readiness Probe and Liveness Probe added to all the head/worker nodes. KubeRay Operator controller watches for Event object changes which can notify in case of readiness probe failures and mark them as Unhealthy . KubeRay Operator controller kills and recreate any Unhealthy Ray head/worker node.","title":"Enable Ray GCS FT"},{"location":"guidance/gcs-ft/#implementation-details","text":"","title":"Implementation Details"},{"location":"guidance/gcs-ft/#readiness-probe-vs-liveness-probe","text":"These are the two types of probes we used in Ray GCS FT. The readiness probe is used to notify KubeRay in case of failures in the corresponding Ray cluster. KubeRay can try its best to recover the Ray cluster. If KubeRay cannot recover the failed head/worker node, the liveness probe gets in, delete the old pod and create a new pod. By default, the liveness probe gets involved later than the readiness probe. The liveness probe is our last resort to recover the Ray cluster. However, in our current implementation, for the readiness probe failures, we also kill & recreate the corresponding pod that runs head/worker node. Currently, the readiness probe and the liveness probe are using the same command to do the work. In the future, we may run different commands for the readiness probe and the liveness probe. On Ray head node, we access a local Ray dashboard http endpoint and a Raylet http endpoint to make sure this head node is in healthy state. Since Ray dashboard does not reside Ray worker node, we only check the local Raylet http endpoint to make sure the worker node is healthy.","title":"Readiness Probe vs Liveness Probe"},{"location":"guidance/gcs-ft/#ray-gcs-ft-annotation","text":"Our Ray GCS FT feature checks if an annotation called ray.io/ft-enabled is set to true in RayCluster YAML file. If so, KubeRay will also add such annotation to the pod whenever the head/worker node is created.","title":"Ray GCS FT Annotation"},{"location":"guidance/gcs-ft/#use-external-redis-cluster","text":"To use external Redis cluster as the backend storage(required by Ray GCS FT), you need to add RAY_REDIS_ADDRESS environment variable to the head node template. Also, you can specify a storage namespace for your Ray cluster by using an annotation ray.io/external-storage-namespace An example can be found at ray-cluster.external-redis.yaml To use SSL/TLS in the connection, you add rediss:// as the prefix of the redis address instead of the redis:// prefix. This feature is only available in Ray 2.2 and above. You can also specify additional environment variables in the head pod to customize the SSL configuration: RAY_REDIS_CA_CERT The location of the CA certificate (optional) RAY_REDIS_CA_PATH Path of trusted certificates (optional) RAY_REDIS_CLIENT_CERT File name of client certificate file (optional) RAY_REDIS_CLIENT_KEY File name of client private key (optional) RAY_REDIS_SERVER_NAME Server name to request (SNI) (optional)","title":"Use External Redis Cluster"},{"location":"guidance/gcs-ft/#kuberay-operator-controller","text":"KubeRay Operator controller watches for new Event reconcile call. If this Event object is to notify the failed readiness probe, controller checks if this pod has ray.io/ft-enabled set to true . If this pod has this annotation set to true, that means this pod belongs to a Ray cluster that has Ray GCS FT enabled. After this, the controller will try to recover the failed pod. If controller cannot recover it, an annotation named ray.io/health-state with a value Unhealthy is added to this pod. In every KubeRay Operator controller reconcile loop, it monitors any pod in Ray cluster that has Unhealthy value in annotation ray.io/health-state . If any pod is found, this pod is deleted and gets recreated.","title":"KubeRay Operator Controller"},{"location":"guidance/gcs-ft/#external-storage-namespace","text":"External storage namespaces can be used to share a single storage backend among multiple Ray clusters. By default, ray.io/external-storage-namespace uses the RayCluster UID as its value when GCS FT is enabled. Or if the user wants to use customized external storage namespace, the user can add ray.io/external-storage-namespace annotation to RayCluster yaml file. Whenever ray.io/external-storage-namespace annotation is set, the head/worker node will have RAY_external_storage_namespace environment variable set which Ray can pick up later.","title":"External Storage Namespace"},{"location":"guidance/gcs-ft/#known-issues-and-limitations","text":"For now, Ray head/worker node that fails the readiness probe recovers itself by restarting itself. More fine-grained control and recovery mechanisms are expected in the future.","title":"Known issues and limitations"},{"location":"guidance/gcs-ft/#test-ray-gcs-ft","text":"Currently, two tests are responsible for ensuring Ray GCS FT is working correctly. Detached actor test RayServe test In detached actor test, a detached actor is created at first. Then, the head node is killed. KubeRay brings back another head node replacement pod. However, the detached actor is still expected to be available. (Note: the client that creates the detached actor does not exist and will retry in case of Ray cluster returns failure) In RayServe test, a simple RayServe app is deployed on the Ray cluster. In case of GCS server crash, the RayServe app continues to be accessible after the head node recovery.","title":"Test Ray GCS FT"},{"location":"guidance/ingress/","text":"Ingress Usage \u00b6 KubeRay built-in ingress support: KubeRay will help users create a Kubernetes ingress when spec.headGroupSpec.enableIngress: true . Currently, the built-in support only supports simple NGINX setups. Note that users still need to install ingress controller by themselves. We do not recommend using built-in ingress support in a production environment with complex routing requirements. Example: NGINX Ingress on KinD (built-in ingress support) Manually setting up an ingress for KubeRay: For production use-cases, we recommend taking this route. Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS Example: Manually setting up NGINX Ingress on KinD Prerequisite \u00b6 It's user's responsibility to install ingress controller by themselves. In order to pass through the customized ingress configuration, you can annotate RayCluster object and the controller will pass the annotations to the ingress object. Example: NGINX Ingress on KinD (built-in ingress support) \u00b6 # Step 1: Create a KinD cluster with `extraPortMappings` and `node-labels` cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF # Step 2: Install NGINX ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s # Step 3: Install KubeRay operator pushd helm-chart/kuberay-operator helm install kuberay-operator . # Step 4: Install RayCluster with NGINX ingress. See https://github.com/ray-project/kuberay/pull/646 # for the explanations of `ray-cluster.ingress.yaml`. Some fields are worth to discuss further: # # (1) metadata.annotations.kubernetes.io/ingress.class: nginx => required # (2) spec.headGroupSpec.enableIngress: true => required # (3) metadata.annotations.nginx.ingress.kubernetes.io/rewrite-target: /$1 => required for NGINX. popd kubectl apply -f ray-operator/config/samples/ray-cluster.ingress.yaml # Step 5: Check ingress created by Step 4. kubectl describe ingress raycluster-ingress-head-ingress # [Example] # ... # Rules: # Host Path Backends # ---- ---- -------- # * # /raycluster-ingress/(.*) raycluster-ingress-head-svc:8265 (10.244.0.11:8265) # Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 # Step 6: Check `<ip>/raycluster-ingress/` on your browser. You will see the Ray Dashboard. # [Note] The forward slash at the end of the address is necessary. `<ip>/raycluster-ingress` # will report \"404 Not Found\". Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS \u00b6 Prerequisite \u00b6 Follow the document Getting started with Amazon EKS \u2013 AWS Management Console and AWS CLI to create an EKS cluster. Follow the installation instructions to set up the AWS Load Balancer controller . Note that the repository maintains a webpage for each release. Please make sure you use the latest installation instructions. (Optional) Try echo server example in the aws-load-balancer-controller repository. (Optional) Read how-it-works.md to understand the mechanism of aws-load-balancer-controller . Instructions \u00b6 # Step 1: Install KubeRay operator and CRD pushd helm-chart/kuberay-operator/ helm install kuberay-operator . popd # Step 2: Install a RayCluster pushd helm-chart/ray-cluster helm install ray-cluster . popd # Step 3: Edit the `ray-operator/config/samples/ray-cluster-alb-ingress.yaml` # # (1) Annotation `alb.ingress.kubernetes.io/subnets` # 1. Please include at least two subnets. # 2. One Availability Zone (ex: us-west-2a) can only have at most 1 subnet. # 3. In this example, you need to select public subnets (subnets that \"Auto-assign public IPv4 address\" is Yes on AWS dashboard) # # (2) Set the name of head pod service to `spec...backend.service.name` eksctl get cluster ${ YOUR_EKS_CLUSTER } # Check subnets on the EKS cluster # Step 4: Create an ALB ingress. When an ingress with proper annotations creates, # AWS Load Balancer controller will reconcile a ALB (not in AWS EKS cluster). kubectl apply -f ray-operator/config/samples/alb-ingress.yaml # Step 5: Check ingress created by Step 4. kubectl describe ingress ray-cluster-ingress # [Example] # Name: ray-cluster-ingress # Labels: <none> # Namespace: default # Address: k8s-default-rayclust-....${REGION_CODE}.elb.amazonaws.com # Default backend: default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>) # Rules: # Host Path Backends # ---- ---- -------- # * # / ray-cluster-kuberay-head-svc:8265 (192.168.185.157:8265) # Annotations: alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/subnets: ${SUBNET_1},${SUBNET_2} # alb.ingress.kubernetes.io/tags: Environment=dev,Team=test # alb.ingress.kubernetes.io/target-type: ip # Events: # Type Reason Age From Message # ---- ------ ---- ---- ------- # Normal SuccessfullyReconciled 39m ingress Successfully reconciled # Step 6: Check ALB on AWS (EC2 -> Load Balancing -> Load Balancers) # The name of the ALB should be like \"k8s-default-rayclust-......\". # Step 7: Check Ray Dashboard by ALB DNS Name. The name of the DNS Name should be like # \"k8s-default-rayclust-.....us-west-2.elb.amazonaws.com\" # Step 8: Delete the ingress, and AWS Load Balancer controller will remove ALB. # Check ALB on AWS to make sure it is removed. kubectl delete ingress ray-cluster-ingress Example: Manually setting up NGINX Ingress on KinD \u00b6 # Step 1: Create a KinD cluster with `extraPortMappings` and `node-labels` # Reference for the setting up of kind cluster: https://kind.sigs.k8s.io/docs/user/ingress/ cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF # Step 2: Install NGINX ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s # Step 3: Install KubeRay operator pushd helm-chart/kuberay-operator helm install kuberay-operator . popd # Step 4: Install RayCluster and create an ingress separately. # If you want to change ingress settings, you can edit the ingress portion in # `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`. # More information about change of setting was documented in https://github.com/ray-project/kuberay/pull/699 # and `ray-operator/config/samples/ray-cluster.separate-ingress.yaml` kubectl apply -f ray-operator/config/samples/ray-cluster.separate-ingress.yaml # Step 5: Check the ingress created in Step 4. kubectl describe ingress raycluster-ingress-head-ingress # [Example] # ... # Rules: # Host Path Backends # ---- ---- -------- # * # /raycluster-ingress/(.*) raycluster-ingress-head-svc:8265 (10.244.0.11:8265) # Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 # Step 6: Check `<ip>/raycluster-ingress/` on your browser. You will see the Ray Dashboard. # [Note] The forward slash at the end of the address is necessary. `<ip>/raycluster-ingress` # will report \"404 Not Found\".","title":"Ingress"},{"location":"guidance/ingress/#ingress-usage","text":"KubeRay built-in ingress support: KubeRay will help users create a Kubernetes ingress when spec.headGroupSpec.enableIngress: true . Currently, the built-in support only supports simple NGINX setups. Note that users still need to install ingress controller by themselves. We do not recommend using built-in ingress support in a production environment with complex routing requirements. Example: NGINX Ingress on KinD (built-in ingress support) Manually setting up an ingress for KubeRay: For production use-cases, we recommend taking this route. Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS Example: Manually setting up NGINX Ingress on KinD","title":"Ingress Usage"},{"location":"guidance/ingress/#prerequisite","text":"It's user's responsibility to install ingress controller by themselves. In order to pass through the customized ingress configuration, you can annotate RayCluster object and the controller will pass the annotations to the ingress object.","title":"Prerequisite"},{"location":"guidance/ingress/#example-nginx-ingress-on-kind-built-in-ingress-support","text":"# Step 1: Create a KinD cluster with `extraPortMappings` and `node-labels` cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF # Step 2: Install NGINX ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s # Step 3: Install KubeRay operator pushd helm-chart/kuberay-operator helm install kuberay-operator . # Step 4: Install RayCluster with NGINX ingress. See https://github.com/ray-project/kuberay/pull/646 # for the explanations of `ray-cluster.ingress.yaml`. Some fields are worth to discuss further: # # (1) metadata.annotations.kubernetes.io/ingress.class: nginx => required # (2) spec.headGroupSpec.enableIngress: true => required # (3) metadata.annotations.nginx.ingress.kubernetes.io/rewrite-target: /$1 => required for NGINX. popd kubectl apply -f ray-operator/config/samples/ray-cluster.ingress.yaml # Step 5: Check ingress created by Step 4. kubectl describe ingress raycluster-ingress-head-ingress # [Example] # ... # Rules: # Host Path Backends # ---- ---- -------- # * # /raycluster-ingress/(.*) raycluster-ingress-head-svc:8265 (10.244.0.11:8265) # Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 # Step 6: Check `<ip>/raycluster-ingress/` on your browser. You will see the Ray Dashboard. # [Note] The forward slash at the end of the address is necessary. `<ip>/raycluster-ingress` # will report \"404 Not Found\".","title":"Example: NGINX Ingress on KinD (built-in ingress support)"},{"location":"guidance/ingress/#example-aws-application-load-balancer-alb-ingress-support-on-aws-eks","text":"","title":"Example: AWS Application Load Balancer (ALB) Ingress support on AWS EKS"},{"location":"guidance/ingress/#prerequisite_1","text":"Follow the document Getting started with Amazon EKS \u2013 AWS Management Console and AWS CLI to create an EKS cluster. Follow the installation instructions to set up the AWS Load Balancer controller . Note that the repository maintains a webpage for each release. Please make sure you use the latest installation instructions. (Optional) Try echo server example in the aws-load-balancer-controller repository. (Optional) Read how-it-works.md to understand the mechanism of aws-load-balancer-controller .","title":"Prerequisite"},{"location":"guidance/ingress/#instructions","text":"# Step 1: Install KubeRay operator and CRD pushd helm-chart/kuberay-operator/ helm install kuberay-operator . popd # Step 2: Install a RayCluster pushd helm-chart/ray-cluster helm install ray-cluster . popd # Step 3: Edit the `ray-operator/config/samples/ray-cluster-alb-ingress.yaml` # # (1) Annotation `alb.ingress.kubernetes.io/subnets` # 1. Please include at least two subnets. # 2. One Availability Zone (ex: us-west-2a) can only have at most 1 subnet. # 3. In this example, you need to select public subnets (subnets that \"Auto-assign public IPv4 address\" is Yes on AWS dashboard) # # (2) Set the name of head pod service to `spec...backend.service.name` eksctl get cluster ${ YOUR_EKS_CLUSTER } # Check subnets on the EKS cluster # Step 4: Create an ALB ingress. When an ingress with proper annotations creates, # AWS Load Balancer controller will reconcile a ALB (not in AWS EKS cluster). kubectl apply -f ray-operator/config/samples/alb-ingress.yaml # Step 5: Check ingress created by Step 4. kubectl describe ingress ray-cluster-ingress # [Example] # Name: ray-cluster-ingress # Labels: <none> # Namespace: default # Address: k8s-default-rayclust-....${REGION_CODE}.elb.amazonaws.com # Default backend: default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>) # Rules: # Host Path Backends # ---- ---- -------- # * # / ray-cluster-kuberay-head-svc:8265 (192.168.185.157:8265) # Annotations: alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/subnets: ${SUBNET_1},${SUBNET_2} # alb.ingress.kubernetes.io/tags: Environment=dev,Team=test # alb.ingress.kubernetes.io/target-type: ip # Events: # Type Reason Age From Message # ---- ------ ---- ---- ------- # Normal SuccessfullyReconciled 39m ingress Successfully reconciled # Step 6: Check ALB on AWS (EC2 -> Load Balancing -> Load Balancers) # The name of the ALB should be like \"k8s-default-rayclust-......\". # Step 7: Check Ray Dashboard by ALB DNS Name. The name of the DNS Name should be like # \"k8s-default-rayclust-.....us-west-2.elb.amazonaws.com\" # Step 8: Delete the ingress, and AWS Load Balancer controller will remove ALB. # Check ALB on AWS to make sure it is removed. kubectl delete ingress ray-cluster-ingress","title":"Instructions"},{"location":"guidance/ingress/#example-manually-setting-up-nginx-ingress-on-kind","text":"# Step 1: Create a KinD cluster with `extraPortMappings` and `node-labels` # Reference for the setting up of kind cluster: https://kind.sigs.k8s.io/docs/user/ingress/ cat <<EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF # Step 2: Install NGINX ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml kubectl wait --namespace ingress-nginx \\ --for = condition = ready pod \\ --selector = app.kubernetes.io/component = controller \\ --timeout = 90s # Step 3: Install KubeRay operator pushd helm-chart/kuberay-operator helm install kuberay-operator . popd # Step 4: Install RayCluster and create an ingress separately. # If you want to change ingress settings, you can edit the ingress portion in # `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`. # More information about change of setting was documented in https://github.com/ray-project/kuberay/pull/699 # and `ray-operator/config/samples/ray-cluster.separate-ingress.yaml` kubectl apply -f ray-operator/config/samples/ray-cluster.separate-ingress.yaml # Step 5: Check the ingress created in Step 4. kubectl describe ingress raycluster-ingress-head-ingress # [Example] # ... # Rules: # Host Path Backends # ---- ---- -------- # * # /raycluster-ingress/(.*) raycluster-ingress-head-svc:8265 (10.244.0.11:8265) # Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 # Step 6: Check `<ip>/raycluster-ingress/` on your browser. You will see the Ray Dashboard. # [Note] The forward slash at the end of the address is necessary. `<ip>/raycluster-ingress` # will report \"404 Not Found\".","title":"Example: Manually setting up NGINX Ingress on KinD"},{"location":"guidance/kuberay-with-MCAD/","text":"KubeRay integration with MCAD (Multi-Cluster-App-Dispatcher) \u00b6 The multi-cluster-app-dispatcher is a Kubernetes controller providing mechanisms for applications to manage batch jobs in a single or multi-cluster environment. For more details please refer here . Use case \u00b6 MCAD allows you to deploy Ray cluster with a guarantee that sufficient resources are available in the cluster prior to actual pod creation in the Kubernetes cluster. It supports features such as: Integrates with upstream Kubernetes scheduling stack for features such co-scheduling, Packing on GPU dimension etc. Ability to wrap any Kubernetes objects. Increases control plane stability by JIT (Just-in Time) object creation. Queuing with policies. Quota management that goes across namespaces. Support for multiple Kubernetes clusters; dispatching jobs to any one of a number of Kubernetes clusters. In order to queue Ray cluster(s) and gang dispatch them when aggregated resources are available please refer to the setup KubeRay-MCAD integration with configuration files here . Submitting KubeRay cluster to MCAD \u00b6 Let's submit two Ray clusters on the same Kubernetes cluster. Assuming you have installed all the pre-requisites mentioned in the KubeRay-MCAD integration , we submit the first Ray cluster using command kubectl create -f aw-raycluster.yaml using config file here . Conditions: Last Transition Micro Time: 2022-09-27T21:07:34.252275Z Last Update Micro Time: 2022-09-27T21:07:34.252273Z Status: True Type: Init Last Transition Micro Time: 2022-09-27T21:07:34.252535Z Last Update Micro Time: 2022-09-27T21:07:34.252534Z Reason: AwaitingHeadOfLine Status: True Type: Queueing Last Transition Micro Time: 2022-09-27T21:07:34.261174Z Last Update Micro Time: 2022-09-27T21:07:34.261174Z Reason: FrontOfQueue. Status: True Type: HeadOfLine Last Transition Micro Time: 2022-09-27T21:07:34.316208Z Last Update Micro Time: 2022-09-27T21:07:34.316208Z Reason: AppWrapperRunnable Status: True Type: Dispatched Controllerfirsttimestamp: 2022-09-27T21:07:34.251877Z Filterignore: true Queuejobstate: Dispatched Sender: before manageQueueJob - afterEtcdDispatching State: Running Events: <none> (base) asmalvan@mcad-dev:~/mcad-kuberay$ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-autoscaler-1-head-9s4x5 2/2 Running 0 47s raycluster-autoscaler-1-worker-small-group-4s6jv 1/1 Running 0 47s As seen the cluster is dispatched and pods are running. Let's submit another Ray cluster and see it queued without creating pending pods using the command kubectl create -f aw-raycluster.yaml . To do this, change cluster name from name: raycluster-autoscaler to name: raycluster-autoscaler-1 and re-submit Conditions: Last Transition Micro Time: 2022-09-27T21:11:06.162080Z Last Update Micro Time: 2022-09-27T21:11:06.162080Z Status: True Type: Init Last Transition Micro Time: 2022-09-27T21:11:06.162401Z Last Update Micro Time: 2022-09-27T21:11:06.162401Z Reason: AwaitingHeadOfLine Status: True Type: Queueing Last Transition Micro Time: 2022-09-27T21:11:06.171619Z Last Update Micro Time: 2022-09-27T21:11:06.171618Z Reason: FrontOfQueue. Status: True Type: HeadOfLine Last Transition Micro Time: 2022-09-27T21:11:06.179694Z Last Update Micro Time: 2022-09-27T21:11:06.179689Z Message: Insufficient resources to dispatch AppWrapper. Reason: AppWrapperNotRunnable. Status: True Type: Backoff Controllerfirsttimestamp: 2022-09-27T21:11:06.161797Z Filterignore: true Queuejobstate: HeadOfLine Sender: before ScheduleNext - setHOL State: Pending Events: <none> As seen the second Ray cluster is queued with no pending pods created. Dispatching policy out of the box is FIFO which can be augmented as per user needs. The second cluster will be dispatched when additional aggregated resources are available in the cluster or the first AppWrapper Ray cluster is deleted.","title":"KubeRay with MCAD"},{"location":"guidance/kuberay-with-MCAD/#kuberay-integration-with-mcad-multi-cluster-app-dispatcher","text":"The multi-cluster-app-dispatcher is a Kubernetes controller providing mechanisms for applications to manage batch jobs in a single or multi-cluster environment. For more details please refer here .","title":"KubeRay integration with MCAD (Multi-Cluster-App-Dispatcher)"},{"location":"guidance/kuberay-with-MCAD/#use-case","text":"MCAD allows you to deploy Ray cluster with a guarantee that sufficient resources are available in the cluster prior to actual pod creation in the Kubernetes cluster. It supports features such as: Integrates with upstream Kubernetes scheduling stack for features such co-scheduling, Packing on GPU dimension etc. Ability to wrap any Kubernetes objects. Increases control plane stability by JIT (Just-in Time) object creation. Queuing with policies. Quota management that goes across namespaces. Support for multiple Kubernetes clusters; dispatching jobs to any one of a number of Kubernetes clusters. In order to queue Ray cluster(s) and gang dispatch them when aggregated resources are available please refer to the setup KubeRay-MCAD integration with configuration files here .","title":"Use case"},{"location":"guidance/kuberay-with-MCAD/#submitting-kuberay-cluster-to-mcad","text":"Let's submit two Ray clusters on the same Kubernetes cluster. Assuming you have installed all the pre-requisites mentioned in the KubeRay-MCAD integration , we submit the first Ray cluster using command kubectl create -f aw-raycluster.yaml using config file here . Conditions: Last Transition Micro Time: 2022-09-27T21:07:34.252275Z Last Update Micro Time: 2022-09-27T21:07:34.252273Z Status: True Type: Init Last Transition Micro Time: 2022-09-27T21:07:34.252535Z Last Update Micro Time: 2022-09-27T21:07:34.252534Z Reason: AwaitingHeadOfLine Status: True Type: Queueing Last Transition Micro Time: 2022-09-27T21:07:34.261174Z Last Update Micro Time: 2022-09-27T21:07:34.261174Z Reason: FrontOfQueue. Status: True Type: HeadOfLine Last Transition Micro Time: 2022-09-27T21:07:34.316208Z Last Update Micro Time: 2022-09-27T21:07:34.316208Z Reason: AppWrapperRunnable Status: True Type: Dispatched Controllerfirsttimestamp: 2022-09-27T21:07:34.251877Z Filterignore: true Queuejobstate: Dispatched Sender: before manageQueueJob - afterEtcdDispatching State: Running Events: <none> (base) asmalvan@mcad-dev:~/mcad-kuberay$ kubectl get pods NAME READY STATUS RESTARTS AGE raycluster-autoscaler-1-head-9s4x5 2/2 Running 0 47s raycluster-autoscaler-1-worker-small-group-4s6jv 1/1 Running 0 47s As seen the cluster is dispatched and pods are running. Let's submit another Ray cluster and see it queued without creating pending pods using the command kubectl create -f aw-raycluster.yaml . To do this, change cluster name from name: raycluster-autoscaler to name: raycluster-autoscaler-1 and re-submit Conditions: Last Transition Micro Time: 2022-09-27T21:11:06.162080Z Last Update Micro Time: 2022-09-27T21:11:06.162080Z Status: True Type: Init Last Transition Micro Time: 2022-09-27T21:11:06.162401Z Last Update Micro Time: 2022-09-27T21:11:06.162401Z Reason: AwaitingHeadOfLine Status: True Type: Queueing Last Transition Micro Time: 2022-09-27T21:11:06.171619Z Last Update Micro Time: 2022-09-27T21:11:06.171618Z Reason: FrontOfQueue. Status: True Type: HeadOfLine Last Transition Micro Time: 2022-09-27T21:11:06.179694Z Last Update Micro Time: 2022-09-27T21:11:06.179689Z Message: Insufficient resources to dispatch AppWrapper. Reason: AppWrapperNotRunnable. Status: True Type: Backoff Controllerfirsttimestamp: 2022-09-27T21:11:06.161797Z Filterignore: true Queuejobstate: HeadOfLine Sender: before ScheduleNext - setHOL State: Pending Events: <none> As seen the second Ray cluster is queued with no pending pods created. Dispatching policy out of the box is FIFO which can be augmented as per user needs. The second cluster will be dispatched when additional aggregated resources are available in the cluster or the first AppWrapper Ray cluster is deleted.","title":"Submitting KubeRay cluster to MCAD"},{"location":"guidance/observability/","text":"Observability \u00b6 RayCluster Status \u00b6 State \u00b6 In the RayCluster resource definition, we use State to represent the current status of the Ray cluster. For now, there are three types of the status exposed by the RayCluster's status.state: ready , unhealthy and failed . | State | Description | | --------- | ----------------------------------------------------------------------------------------------- | | ready | The Ray cluster is ready for use. | | unhealthy | The rayStartParams are misconfigured and the Ray cluster may not function properly. | | failed | A severe issue has prevented the head node or worker nodes from starting. | If you use the apiserver to retrieve the resource, you may find the state in the clusterState field. curl -- reques t GET '<baseUrl>/apis/v 1 alpha 2 / na mespaces/< na mespace>/clus ters /<rayclus ter - na me>' { \"name\" : \"<raycluster-name>\" , \"namespace\" : \"<namespace>\" , //... \"createdAt\" : \"2022-08-10T10:31:25Z\" , \"clusterState\" : \"ready\" , //... } Endpoint \u00b6 If you use the nodeport as service to expose the raycluster endpoint, like dashboard or redis, there are endpoints field in the status to record the service endpoints. you can directly use the ports in the endpoints to connect to the related service. Also, if you use apiserver to retrieve the resource, you can find the endpoints in the serviceEndpoint field. curl -- reques t GET '<baseUrl>/apis/v 1 alpha 2 / na mespaces/< na mespace>/clus ters /<rayclus ter - na me>' { \"name\" : \"<raycluster-name>\" , \"namespace\" : \"<namespace>\" , //... \"serviceEndpoint\" : { \"dashboard\" : \"30001\" , \"head\" : \"30002\" , \"metrics\" : \"30003\" , \"redis\" : \"30004\" }, //... } Ray Cluster: Monitoring with Prometheus & Grafana \u00b6 In this section we will describe how to monitor Ray Clusters in Kubernetes using Prometheus & Grafana. We also leverage the Prometheus Operator to start the whole monitoring system. Requirements: - Prometheus deployed in Kubernetes - Required CRD: servicemonitors.monitoring.coreos.com - Requered CRD: podmonitors.monitoring.coreos.com - Grafana up and running Enable Ray Cluster Metrics \u00b6 Before we define any Prometheus objects, let us first enable and export metrics to a specific port. To enable ray metrics on Head node or a worker node, we need to pass the following option --metrics-expose-port=9001 . We can set the specific option by adding metrics-export-port: \"9001\" to the head node & worker nodes in the rayclusters.ray.io manifest. We also need to export port 9001 in the head node & worker nodes apiVersion : ray.io/v1alpha1 kind : RayCluster metadata : ... name : ray-cluster spec : enableInTreeAutoscaling : true headGroupSpec : rayStartParams : ... metrics-export-port : \"9001\" <--- Enable for the head node ... template : metadata : ... spec : ... containers : - ports : - containerPort : 10001 name : client protocol : TCP - containerPort : 8265 name : dashboard protocol : TCP - containerPort : 8000 name : ray-serve protocol : TCP - containerPort : 6379 name : redis protocol : TCP - containerPort : 9001 name : metrics protocol : TCP workerGroupSpecs : - groupName : workergroup ... rayStartParams : ... metrics-export-port : \"9001\" <--- Enable for worker nodes ... template : metadata : ... spec : ... containers : - ports : - containerPort : 9001 name : metrics protocol : TCP ... ... If you use $kuberay/helm-chart/ray-cluster , then you can add it in the values.yaml head : groupName : headgroup ... initArgs : metrics-export-port : \"9001\" <--- Enable for the head node ... ports : - containerPort : 10001 protocol : TCP name : \"client\" - containerPort : 8265 protocol : TCP name : \"dashboard\" - containerPort : 8000 protocol : TCP name : \"ray-serve\" - containerPort : 6379 protocol : TCP name : \"redis\" - containerPort : 9001 <--- Enable this port protocol : TCP name : \"metrics\" ... worker : groupName : workergroup ... initArgs : ... metrics-export-port : \"9001\" <--- Enable for the head node ports : - containerPort : 9001 <--- Enable this port protocol : TCP name : \"metrics\" ... ... Deploying the cluster with the above options should export metrics on port 9001 . To check, we can port-forward port 9001 to our localhost and query via curl. k port-forward <ray-head-node-id> 9001 :9001 From a second terminal issue $> curl localhost:9001 # TYPE ray_pull_manager_object_request_time_ms histogram ... ray_pull_manager_object_request_time_ms_sum { Component = \"raylet\" ,... ... Before we move on, first ensure that the required metrics port is also defined in the Ray's cluster Kubernetes service. This is done automatically via the Ray Operator if you define the metrics port containerPort: 9001 along with the name and protocol. $> kubectl get svc <ray-cluster-name>-head-svc -o yaml NAME TYPE ... PORT ( S ) ... ... ClusterIP ... 6379 /TCP,9001/TCP,10001/TCP,8265/TCP,8000/TCP ... We are now ready to create the required Prometheus CRDs to collect metrics Collect Head Node metrics with ServiceMonitors \u00b6 Prometheus provides a CRD that targets Kubernetes services to collect metrics. The idea is that we will define a CRD that will have selectors that match the Ray Cluster Kubernetes service labels and ports, the metrics port. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : <ray-cluster-name>-head-monitor <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespace : <ray-cluster-namespace> <-- Add the namespace of your ray cluster spec : endpoints : - interval : 1m path : /metrics scrapeTimeout : 10s port : metrics jobLabel : <ray-cluster-name>-ray-head <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespaceSelector : matchNames : - <ray-cluster-namespace> <-- Add the namespace of your ray cluster selector : matchLabels : ray.io/cluster : <ray-cluster-name> <-- Replace <ray-cluster-name> with the actual Ray Cluster name ray.io/identifier : <ray-cluster-name>-head <-- Replace <ray-cluster-name> with the actual Ray Cluster name ray.io/node-type : head targetLabels : - ray.io/cluster A notes for the targetLabels . We added spec.targetLabels[0].ray.io/cluster because we want to include the name of the ray cluster in the metrics that will be generated by this service monitor. The ray.io/cluster label is part of the Ray head node service and it will be transformed to a ray_io_cluster metric label. That is, any metric that will be imported, will also container the following label ray_io_cluster=<ray-cluster-name> . This may seem like optional but it becomes mandatory if you deploy multiple ray clusters. Create the above service monitor by issuing k apply -f serviceMonitor.yaml After a while, Prometheus should start scraping metrics from the head node. You can confirm that by visiting the Prometheus web ui and start typing ray_ . Prometheus should create a dropdown list with suggested Ray metrics. curl 'https://<prometheus-endpoint>/api/v1/query?query=ray_object_store_available_memory' -H 'Accept: */*' Collect Worker Node metrics with PodMonitors \u00b6 Ray operator does not create a Kubernetes service for the ray workers, therefore we can not use a Prometheus ServiceMonitors to scrape the metrics from our workers. Note : We could create a Kubernetes service with selectors a common label subset from our worker pods, however this is not ideal because our workers are independent from each other, that is, they are not a collection of replicas spawned by replicaset controller. Due to that, we should avoid using a Kubernetes service for grouping them together. To collect worker metrics, we can use Prometheus PodMonitros CRD . apiVersion : monitoring.coreos.com/v1 kind : PodMonitor metadata : labels : ray.io/cluster : <ray-cluster-name> <-- Replace <ray-cluster-name> with the actual Ray Cluster name name : <ray-cluster-name>-workers-monitor <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespace : <ray-cluster-namespace> <-- Add the namespace of your ray cluster spec : jobLabel : <ray-cluster-name>-ray-workers <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespaceSelector : matchNames : - <ray-cluster-namespace> <-- Add the namespace of your ray cluster podMetricsEndpoints : - interval : 30s port : metrics scrapeTimeout : 10s podTargetLabels : - ray.io/cluster selector : matchLabels : ray.io/is-ray-node : \"yes\" ray.io/node-type : worker Since we are not selecting a Kubernetes service but pods, our matchLabels now define a set of labels that is common on all Ray workers. We also define metadata.labels by manually adding ray.io/cluster: <ray-cluster-name> and then instructing the PodMonitors resource to add that label in the scraped metrics via spec.podTargetLabels[0].ray.io/cluster . Apply the above PodMonitor manifest k apply -f podMonitor.yaml Last, wait a bit and then ensure that you can see Ray worker metrics in Prometheus curl 'https://<prometheus-endpoint>/api/v1/query?query=ray_object_store_available_memory' -H 'Accept: */*' The above http query should yield metrics from the head node and your worker nodes We have everything we need now and we can use Grafana to create some panels and visualize the scrapped metrics Grafana: Visualize ingested Ray metrics \u00b6 You can use the json in config/grafana to import in Grafana the Ray dashboards. Custom Metrics & Alerting \u00b6 We can also define custom metrics, and create alerts by using prometheusrules.monitoring.coreos.com CRD. Because custom metrics, and alerting is different for each team and setup, we have included an example under $kuberay/config/prometheus/rules that you can use to build custom metrics and alerts","title":"Observability"},{"location":"guidance/observability/#observability","text":"","title":"Observability"},{"location":"guidance/observability/#raycluster-status","text":"","title":"RayCluster Status"},{"location":"guidance/observability/#state","text":"In the RayCluster resource definition, we use State to represent the current status of the Ray cluster. For now, there are three types of the status exposed by the RayCluster's status.state: ready , unhealthy and failed . | State | Description | | --------- | ----------------------------------------------------------------------------------------------- | | ready | The Ray cluster is ready for use. | | unhealthy | The rayStartParams are misconfigured and the Ray cluster may not function properly. | | failed | A severe issue has prevented the head node or worker nodes from starting. | If you use the apiserver to retrieve the resource, you may find the state in the clusterState field. curl -- reques t GET '<baseUrl>/apis/v 1 alpha 2 / na mespaces/< na mespace>/clus ters /<rayclus ter - na me>' { \"name\" : \"<raycluster-name>\" , \"namespace\" : \"<namespace>\" , //... \"createdAt\" : \"2022-08-10T10:31:25Z\" , \"clusterState\" : \"ready\" , //... }","title":"State"},{"location":"guidance/observability/#endpoint","text":"If you use the nodeport as service to expose the raycluster endpoint, like dashboard or redis, there are endpoints field in the status to record the service endpoints. you can directly use the ports in the endpoints to connect to the related service. Also, if you use apiserver to retrieve the resource, you can find the endpoints in the serviceEndpoint field. curl -- reques t GET '<baseUrl>/apis/v 1 alpha 2 / na mespaces/< na mespace>/clus ters /<rayclus ter - na me>' { \"name\" : \"<raycluster-name>\" , \"namespace\" : \"<namespace>\" , //... \"serviceEndpoint\" : { \"dashboard\" : \"30001\" , \"head\" : \"30002\" , \"metrics\" : \"30003\" , \"redis\" : \"30004\" }, //... }","title":"Endpoint"},{"location":"guidance/observability/#ray-cluster-monitoring-with-prometheus-grafana","text":"In this section we will describe how to monitor Ray Clusters in Kubernetes using Prometheus & Grafana. We also leverage the Prometheus Operator to start the whole monitoring system. Requirements: - Prometheus deployed in Kubernetes - Required CRD: servicemonitors.monitoring.coreos.com - Requered CRD: podmonitors.monitoring.coreos.com - Grafana up and running","title":"Ray Cluster: Monitoring with Prometheus &amp; Grafana"},{"location":"guidance/observability/#enable-ray-cluster-metrics","text":"Before we define any Prometheus objects, let us first enable and export metrics to a specific port. To enable ray metrics on Head node or a worker node, we need to pass the following option --metrics-expose-port=9001 . We can set the specific option by adding metrics-export-port: \"9001\" to the head node & worker nodes in the rayclusters.ray.io manifest. We also need to export port 9001 in the head node & worker nodes apiVersion : ray.io/v1alpha1 kind : RayCluster metadata : ... name : ray-cluster spec : enableInTreeAutoscaling : true headGroupSpec : rayStartParams : ... metrics-export-port : \"9001\" <--- Enable for the head node ... template : metadata : ... spec : ... containers : - ports : - containerPort : 10001 name : client protocol : TCP - containerPort : 8265 name : dashboard protocol : TCP - containerPort : 8000 name : ray-serve protocol : TCP - containerPort : 6379 name : redis protocol : TCP - containerPort : 9001 name : metrics protocol : TCP workerGroupSpecs : - groupName : workergroup ... rayStartParams : ... metrics-export-port : \"9001\" <--- Enable for worker nodes ... template : metadata : ... spec : ... containers : - ports : - containerPort : 9001 name : metrics protocol : TCP ... ... If you use $kuberay/helm-chart/ray-cluster , then you can add it in the values.yaml head : groupName : headgroup ... initArgs : metrics-export-port : \"9001\" <--- Enable for the head node ... ports : - containerPort : 10001 protocol : TCP name : \"client\" - containerPort : 8265 protocol : TCP name : \"dashboard\" - containerPort : 8000 protocol : TCP name : \"ray-serve\" - containerPort : 6379 protocol : TCP name : \"redis\" - containerPort : 9001 <--- Enable this port protocol : TCP name : \"metrics\" ... worker : groupName : workergroup ... initArgs : ... metrics-export-port : \"9001\" <--- Enable for the head node ports : - containerPort : 9001 <--- Enable this port protocol : TCP name : \"metrics\" ... ... Deploying the cluster with the above options should export metrics on port 9001 . To check, we can port-forward port 9001 to our localhost and query via curl. k port-forward <ray-head-node-id> 9001 :9001 From a second terminal issue $> curl localhost:9001 # TYPE ray_pull_manager_object_request_time_ms histogram ... ray_pull_manager_object_request_time_ms_sum { Component = \"raylet\" ,... ... Before we move on, first ensure that the required metrics port is also defined in the Ray's cluster Kubernetes service. This is done automatically via the Ray Operator if you define the metrics port containerPort: 9001 along with the name and protocol. $> kubectl get svc <ray-cluster-name>-head-svc -o yaml NAME TYPE ... PORT ( S ) ... ... ClusterIP ... 6379 /TCP,9001/TCP,10001/TCP,8265/TCP,8000/TCP ... We are now ready to create the required Prometheus CRDs to collect metrics","title":"Enable Ray Cluster Metrics"},{"location":"guidance/observability/#collect-head-node-metrics-with-servicemonitors","text":"Prometheus provides a CRD that targets Kubernetes services to collect metrics. The idea is that we will define a CRD that will have selectors that match the Ray Cluster Kubernetes service labels and ports, the metrics port. apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : <ray-cluster-name>-head-monitor <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespace : <ray-cluster-namespace> <-- Add the namespace of your ray cluster spec : endpoints : - interval : 1m path : /metrics scrapeTimeout : 10s port : metrics jobLabel : <ray-cluster-name>-ray-head <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespaceSelector : matchNames : - <ray-cluster-namespace> <-- Add the namespace of your ray cluster selector : matchLabels : ray.io/cluster : <ray-cluster-name> <-- Replace <ray-cluster-name> with the actual Ray Cluster name ray.io/identifier : <ray-cluster-name>-head <-- Replace <ray-cluster-name> with the actual Ray Cluster name ray.io/node-type : head targetLabels : - ray.io/cluster A notes for the targetLabels . We added spec.targetLabels[0].ray.io/cluster because we want to include the name of the ray cluster in the metrics that will be generated by this service monitor. The ray.io/cluster label is part of the Ray head node service and it will be transformed to a ray_io_cluster metric label. That is, any metric that will be imported, will also container the following label ray_io_cluster=<ray-cluster-name> . This may seem like optional but it becomes mandatory if you deploy multiple ray clusters. Create the above service monitor by issuing k apply -f serviceMonitor.yaml After a while, Prometheus should start scraping metrics from the head node. You can confirm that by visiting the Prometheus web ui and start typing ray_ . Prometheus should create a dropdown list with suggested Ray metrics. curl 'https://<prometheus-endpoint>/api/v1/query?query=ray_object_store_available_memory' -H 'Accept: */*'","title":"Collect Head Node metrics with ServiceMonitors"},{"location":"guidance/observability/#collect-worker-node-metrics-with-podmonitors","text":"Ray operator does not create a Kubernetes service for the ray workers, therefore we can not use a Prometheus ServiceMonitors to scrape the metrics from our workers. Note : We could create a Kubernetes service with selectors a common label subset from our worker pods, however this is not ideal because our workers are independent from each other, that is, they are not a collection of replicas spawned by replicaset controller. Due to that, we should avoid using a Kubernetes service for grouping them together. To collect worker metrics, we can use Prometheus PodMonitros CRD . apiVersion : monitoring.coreos.com/v1 kind : PodMonitor metadata : labels : ray.io/cluster : <ray-cluster-name> <-- Replace <ray-cluster-name> with the actual Ray Cluster name name : <ray-cluster-name>-workers-monitor <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespace : <ray-cluster-namespace> <-- Add the namespace of your ray cluster spec : jobLabel : <ray-cluster-name>-ray-workers <-- Replace <ray-cluster-name> with the actual Ray Cluster name namespaceSelector : matchNames : - <ray-cluster-namespace> <-- Add the namespace of your ray cluster podMetricsEndpoints : - interval : 30s port : metrics scrapeTimeout : 10s podTargetLabels : - ray.io/cluster selector : matchLabels : ray.io/is-ray-node : \"yes\" ray.io/node-type : worker Since we are not selecting a Kubernetes service but pods, our matchLabels now define a set of labels that is common on all Ray workers. We also define metadata.labels by manually adding ray.io/cluster: <ray-cluster-name> and then instructing the PodMonitors resource to add that label in the scraped metrics via spec.podTargetLabels[0].ray.io/cluster . Apply the above PodMonitor manifest k apply -f podMonitor.yaml Last, wait a bit and then ensure that you can see Ray worker metrics in Prometheus curl 'https://<prometheus-endpoint>/api/v1/query?query=ray_object_store_available_memory' -H 'Accept: */*' The above http query should yield metrics from the head node and your worker nodes We have everything we need now and we can use Grafana to create some panels and visualize the scrapped metrics","title":"Collect Worker Node metrics with PodMonitors"},{"location":"guidance/observability/#grafana-visualize-ingested-ray-metrics","text":"You can use the json in config/grafana to import in Grafana the Ray dashboards.","title":"Grafana: Visualize ingested Ray metrics"},{"location":"guidance/observability/#custom-metrics-alerting","text":"We can also define custom metrics, and create alerts by using prometheusrules.monitoring.coreos.com CRD. Because custom metrics, and alerting is different for each team and setup, we have included an example under $kuberay/config/prometheus/rules that you can use to build custom metrics and alerts","title":"Custom Metrics &amp; Alerting"},{"location":"guidance/rayjob/","text":"Ray Job (alpha) \u00b6 Note: This is the alpha version of Ray Job Support in KubeRay. There will be ongoing improvements for Ray Job in the future releases. Prerequisites \u00b6 Ray 1.10 or higher KubeRay v0.3.0 What is a RayJob? \u00b6 RayJob is a new custom resource (CR) supported by KubeRay in v0.3.0. A RayJob manages 2 things: * Ray Cluster: Manages resources in a Kubernetes cluster. * Job: Manages jobs in a Ray Cluster. What does the RayJob provide? \u00b6 Kubernetes-native support for Ray clusters and Ray Jobs. You can use a Kubernetes config to define a Ray cluster and job, and use kubectl to create them. The cluster can be deleted automatically once the job is finished. Deploy KubeRay \u00b6 Make sure KubeRay v0.3.0 version is deployed in your cluster. For installation instructions, please follow the documentation . Run an example Job \u00b6 There is one example config file to deploy a RayJob included here: ray_v1alpha1_rayjob.yaml # Create a RayJob. $ kubectl apply -f config/samples/ray_v1alpha1_rayjob.yaml # List running RayJobs. $ kubectl get rayjob NAME AGE rayjob-sample 7s # RayJob sample will also create a raycluster. # raycluster will create few resources including pods and services. You can use the following commands to check them: $ kubectl get rayclusters $ kubectl get pod RayJob Configuration \u00b6 entrypoint - The shell command to run for this job. job_id. jobId - (Optional) Job ID to specify for the job. If not provided, one will be generated. metadata - Arbitrary user-provided metadata for the job. runtimeEnv - base64 string of the runtime json string. shutdownAfterJobFinishes - whether to recycle the cluster after job finishes. ttlSecondsAfterFinished - TTL to clean up the cluster. This only works if shutdownAfterJobFinishes is set. RayJob Observability \u00b6 You can use kubectl logs to check the operator logs or the head/worker nodes logs. You can also use kubectl describe rayjobs rayjob-sample to check the states and event logs of your RayJob instance: Status: Dashboard URL: rayjob-sample-raycluster-vnl8w-head-svc.ray-system.svc.cluster.local:8265 End Time: 2022-07-24T02:04:56Z Job Deployment Status: Complete Job Id: test-hehe Job Status: SUCCEEDED Message: Job finished successfully. Ray Cluster Name: rayjob-sample-raycluster-vnl8w Ray Cluster Status: Available Worker Replicas: 1 Endpoints: Client: 32572 Dashboard: 32276 Gcs - Server: 30679 Last Update Time: 2022-07-24T02:04:43Z State: ready Start Time: 2022-07-24T02:04:49Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 90s rayjob-controller Created cluster rayjob-sample-raycluster-vnl8w Normal Submitted 82s rayjob-controller Submit Job test-hehe Normal Deleted 15s rayjob-controller Deleted cluster rayjob-sample-raycluster-vnl8w If the job doesn't run successfully, the above describe command will provide information about that too: Status: Dashboard URL: rayjob-sample-raycluster-nrdm8-head-svc.ray-system.svc.cluster.local:8265 End Time: 2022-07-24T02:01:39Z Job Deployment Status: Complete Job Id: test-hehe Job Status: FAILED Message: Job failed due to an application error, last available logs: python: can't open file '/tmp/code/script.ppy': [Errno 2] No such file or directory Ray Cluster Name: rayjob-sample-raycluster-nrdm8 Ray Cluster Status: Available Worker Replicas: 1 Endpoints: Client: 31852 Dashboard: 32606 Gcs - Server: 32436 Last Update Time: 2022-07-24T02:01:30Z State: ready Start Time: 2022-07-24T02:01:38Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 2m9s rayjob-controller Created cluster rayjob-sample-raycluster-nrdm8 Normal Submitted 2m rayjob-controller Submit Job test-hehe Normal Deleted 58s rayjob-controller Deleted cluster rayjob-sample-raycluster-nrdm8 Delete the RayJob instance \u00b6 $ kubectl delete -f config/samples/ray_v1alpha1_rayjob.yaml","title":"RayJob"},{"location":"guidance/rayjob/#ray-job-alpha","text":"Note: This is the alpha version of Ray Job Support in KubeRay. There will be ongoing improvements for Ray Job in the future releases.","title":"Ray Job (alpha)"},{"location":"guidance/rayjob/#prerequisites","text":"Ray 1.10 or higher KubeRay v0.3.0","title":"Prerequisites"},{"location":"guidance/rayjob/#what-is-a-rayjob","text":"RayJob is a new custom resource (CR) supported by KubeRay in v0.3.0. A RayJob manages 2 things: * Ray Cluster: Manages resources in a Kubernetes cluster. * Job: Manages jobs in a Ray Cluster.","title":"What is a RayJob?"},{"location":"guidance/rayjob/#what-does-the-rayjob-provide","text":"Kubernetes-native support for Ray clusters and Ray Jobs. You can use a Kubernetes config to define a Ray cluster and job, and use kubectl to create them. The cluster can be deleted automatically once the job is finished.","title":"What does the RayJob provide?"},{"location":"guidance/rayjob/#deploy-kuberay","text":"Make sure KubeRay v0.3.0 version is deployed in your cluster. For installation instructions, please follow the documentation .","title":"Deploy KubeRay"},{"location":"guidance/rayjob/#run-an-example-job","text":"There is one example config file to deploy a RayJob included here: ray_v1alpha1_rayjob.yaml # Create a RayJob. $ kubectl apply -f config/samples/ray_v1alpha1_rayjob.yaml # List running RayJobs. $ kubectl get rayjob NAME AGE rayjob-sample 7s # RayJob sample will also create a raycluster. # raycluster will create few resources including pods and services. You can use the following commands to check them: $ kubectl get rayclusters $ kubectl get pod","title":"Run an example Job"},{"location":"guidance/rayjob/#rayjob-configuration","text":"entrypoint - The shell command to run for this job. job_id. jobId - (Optional) Job ID to specify for the job. If not provided, one will be generated. metadata - Arbitrary user-provided metadata for the job. runtimeEnv - base64 string of the runtime json string. shutdownAfterJobFinishes - whether to recycle the cluster after job finishes. ttlSecondsAfterFinished - TTL to clean up the cluster. This only works if shutdownAfterJobFinishes is set.","title":"RayJob Configuration"},{"location":"guidance/rayjob/#rayjob-observability","text":"You can use kubectl logs to check the operator logs or the head/worker nodes logs. You can also use kubectl describe rayjobs rayjob-sample to check the states and event logs of your RayJob instance: Status: Dashboard URL: rayjob-sample-raycluster-vnl8w-head-svc.ray-system.svc.cluster.local:8265 End Time: 2022-07-24T02:04:56Z Job Deployment Status: Complete Job Id: test-hehe Job Status: SUCCEEDED Message: Job finished successfully. Ray Cluster Name: rayjob-sample-raycluster-vnl8w Ray Cluster Status: Available Worker Replicas: 1 Endpoints: Client: 32572 Dashboard: 32276 Gcs - Server: 30679 Last Update Time: 2022-07-24T02:04:43Z State: ready Start Time: 2022-07-24T02:04:49Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 90s rayjob-controller Created cluster rayjob-sample-raycluster-vnl8w Normal Submitted 82s rayjob-controller Submit Job test-hehe Normal Deleted 15s rayjob-controller Deleted cluster rayjob-sample-raycluster-vnl8w If the job doesn't run successfully, the above describe command will provide information about that too: Status: Dashboard URL: rayjob-sample-raycluster-nrdm8-head-svc.ray-system.svc.cluster.local:8265 End Time: 2022-07-24T02:01:39Z Job Deployment Status: Complete Job Id: test-hehe Job Status: FAILED Message: Job failed due to an application error, last available logs: python: can't open file '/tmp/code/script.ppy': [Errno 2] No such file or directory Ray Cluster Name: rayjob-sample-raycluster-nrdm8 Ray Cluster Status: Available Worker Replicas: 1 Endpoints: Client: 31852 Dashboard: 32606 Gcs - Server: 32436 Last Update Time: 2022-07-24T02:01:30Z State: ready Start Time: 2022-07-24T02:01:38Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 2m9s rayjob-controller Created cluster rayjob-sample-raycluster-nrdm8 Normal Submitted 2m rayjob-controller Submit Job test-hehe Normal Deleted 58s rayjob-controller Deleted cluster rayjob-sample-raycluster-nrdm8","title":"RayJob Observability"},{"location":"guidance/rayjob/#delete-the-rayjob-instance","text":"$ kubectl delete -f config/samples/ray_v1alpha1_rayjob.yaml","title":"Delete the RayJob instance"},{"location":"guidance/rayservice/","text":"Ray Services (alpha) \u00b6 Note: This is the alpha version of Ray Services. There will be ongoing improvements for Ray Services in the future releases. Prerequisite \u00b6 Ray 2.0 is required. What is a RayService? \u00b6 RayService is a new custom resource (CR) supported by KubeRay in v0.3.0. A RayService manages 2 things: Ray Cluster : Manages resources in a Kubernetes cluster. Ray Serve Deployment Graph : Manages users' deployment graphs. What does the RayService provide? \u00b6 Kubernetes-native support for Ray clusters and Ray Serve deployment graphs. After using a Kubernetes config to define a Ray cluster and its Ray Serve deployment graphs, you can use kubectl to create the cluster and its graphs. In-place update for Ray Serve deployment graph. Users can update the Ray Serve deployment graph config in the RayService CR config and use kubectl apply to update the deployment graph. Zero downtime upgrade for Ray clusters. Users can update the Ray cluster config in the RayService CR config and use kubectl apply to update the cluster. RayService will temporarily create a pending cluster and wait for it to be ready, then switch traffic to the new cluster and terminate the old one. Services HA. RayService will monitor the Ray cluster and Serve deployments' health statuses. If RayService detects an unhealthy status for a period of time, RayService will try to create a new Ray cluster and switch traffic to the new cluster when it is ready. Deploy the Operator \u00b6 $ kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.3.0&timeout=90s\" Check that the controller is running. $ kubectl get deployments -n ray-system NAME READY UP-TO-DATE AVAILABLE AGE ray-operator 1 /1 1 1 40s $ kubectl get pods -n ray-system NAME READY STATUS RESTARTS AGE ray-operator-75dbbf8587-5lrvn 1 /1 Running 0 31s Run an Example Cluster \u00b6 An example config file to deploy RayService is included here: ray_v1alpha1_rayservice.yaml # Create a ray service and deploy fruit deployment graph. $ kubectl apply -f config/samples/ray_v1alpha1_rayservice.yaml # List running RayServices. $ kubectl get rayservice NAME AGE rayservice-sample 7s # The created RayService should include a head pod, a worker pod, and four services. $ kubectl get pods NAME READY STATUS RESTARTS AGE ervice-sample-raycluster-qd2vl-worker-small-group-bxpp6 1 /1 Running 0 24m rayservice-sample-raycluster-qd2vl-head-45hj4 1 /1 Running 0 24m $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 62d # A head node service maintained by the RayService. rayservice-sample-head-svc ClusterIP 10 .100.34.24 <none> 6379 /TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP 24m # A dashboard agent service maintained by the RayCluster. rayservice-sample-raycluster-qd2vl-dashboard-svc ClusterIP 10 .100.109.177 <none> 52365 /TCP 24m # A head node service maintained by the RayCluster. rayservice-sample-raycluster-qd2vl-head-svc ClusterIP 10 .100.180.221 <none> 6379 /TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP 24m # A serve service maintained by the RayService. rayservice-sample-serve-svc ClusterIP 10 .100.39.92 <none> 8000 /TCP 24m Note: Default ports and their definitions. Port Definition 6379 Ray GCS 8265 Ray Dashboard 10001 Ray Client 8000 Ray Serve 52365 Ray Dashboard Agent Get information about the RayService using its name. $ kubectl describe rayservices rayservice-sample Access User Services \u00b6 The users' traffic can go through the serve service (e.g. rayservice-sample-serve-svc ). Run a Curl Pod \u00b6 $ kubectl run curl --image = radial/busyboxplus:curl -i --tty Or if you already have a curl pod running, you can login using kubectl exec -it curl sh . For the fruit example deployment, you can try the following request: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000 -d '[\"MANGO\", 2]' > 6 You should get the response 6 . Use Port Forwarding \u00b6 Set up Kubernetes port forwarding. $ kubectl port-forward service/rayservice-sample-serve-svc 8000 For the fruit example deployment, you can try the following request: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' localhost:8000 -d '[\"MANGO\", 2]' > 6 Note: serve-svc is HA in general. It will do traffic routing among all the workers which have serve deployments and will always try to point to the healthy cluster, even during upgrading or failing cases. You can set serviceUnhealthySecondThreshold to define the threshold of seconds that the serve deployments fail. You can also set deploymentUnhealthySecondThreshold to define the threshold of seconds that Ray fails to deploy any serve deployments. Access Ray Dashboard \u00b6 Set up Kubernetes port forwarding for the dashboard. $ kubectl port-forward service/rayservice-sample-head-svc 8265 Access the dashboard using a web browser at localhost:8265 . Update Ray Serve Deployment Graph \u00b6 You can update the serveConfig in your RayService config file. For example, update the price of mangos to 4 in ray_v1alpha1_rayservice.yaml : - name: MangoStand numReplicas: 1 userConfig: | price: 4 Use kubectl apply to update your RayService and kubectl describe rayservices rayservice-sample to take a look at the RayService's information. It should look similar to: serveDeploymentStatuses: - healthLastUpdateTime: \"2022-07-18T21:51:37Z\" lastUpdateTime: \"2022-07-18T21:51:41Z\" name: MangoStand status: UPDATING After it finishes deployment, let's send a request again. In the curl pod from earlier, run: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000 -d '[\"MANGO\", 2]' > 8 Or if using port forwarding: curl -X POST -H 'Content-Type: application/json' localhost:8000 -d '[\"MANGO\", 2]' > 8 You should now get 8 as a result. Upgrade RayService RayCluster Config \u00b6 You can update the rayClusterConfig in your RayService config file. For example, you can increase the number of workers to 2: workerGroupSpecs: # the pod replicas in this group typed worker - replicas: 2 Use kubectl apply to update your RayService and kubectl describe rayservices rayservice-sample to take a look at the RayService's information. It should look similar to: pendingServiceStatus: appStatus: {} dashboardStatus: healthLastUpdateTime: \"2022-07-18T21:54:53Z\" lastUpdateTime: \"2022-07-18T21:54:54Z\" rayClusterName: rayservice-sample-raycluster-bshfr rayClusterStatus: {} You can see the RayService is preparing a pending cluster. Once the pending cluster is healthy, the RayService will make it the active cluster and terminate the previous one. RayService Observability \u00b6 You can use kubectl logs to check the operator logs or the head/worker nodes logs. You can also use kubectl describe rayservices rayservice-sample to check the states and event logs of your RayService instance. For Ray Serve monitoring, you can refer to the Ray observability documentation . To run Ray state APIs, log in to the head pod by running kubectl exec -it <head-node-pod> bash and use the Ray CLI or you can run commands locally using kubectl exec -it <head-node-pod> -- <ray state api> . For example, kubectl exec -it <head-node-pod> -- ray summary tasks outputs the following: ======== Tasks Summary: 2022 -07-28 15 :10:24.801670 ======== Stats: ------------------------------------ total_actor_scheduled: 17 total_actor_tasks: 5 total_tasks: 0 Table ( group by func_name ) : ------------------------------------ FUNC_OR_CLASS_NAME STATE_COUNTS TYPE 0 ServeController.listen_for_change RUNNING: 5 ACTOR_TASK 1 ServeReplica:MangoStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 2 HTTPProxyActor.__init__ FINISHED: 2 ACTOR_CREATION_TASK 3 ServeReplica:PearStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 4 ServeReplica:OrangeStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 5 ServeReplica:FruitMarket.__init__ FINISHED: 3 ACTOR_CREATION_TASK 6 ServeReplica:DAGDriver.__init__ FINISHED: 2 ACTOR_CREATION_TASK 7 ServeController.__init__ FINISHED: 1 ACTOR_CREATION_TASK Delete the RayService Instance \u00b6 $ kubectl delete -f config/samples/ray_v1alpha1_rayservice.yaml Delete the Operator \u00b6 $ kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\"","title":"RayService"},{"location":"guidance/rayservice/#ray-services-alpha","text":"Note: This is the alpha version of Ray Services. There will be ongoing improvements for Ray Services in the future releases.","title":"Ray Services (alpha)"},{"location":"guidance/rayservice/#prerequisite","text":"Ray 2.0 is required.","title":"Prerequisite"},{"location":"guidance/rayservice/#what-is-a-rayservice","text":"RayService is a new custom resource (CR) supported by KubeRay in v0.3.0. A RayService manages 2 things: Ray Cluster : Manages resources in a Kubernetes cluster. Ray Serve Deployment Graph : Manages users' deployment graphs.","title":"What is a RayService?"},{"location":"guidance/rayservice/#what-does-the-rayservice-provide","text":"Kubernetes-native support for Ray clusters and Ray Serve deployment graphs. After using a Kubernetes config to define a Ray cluster and its Ray Serve deployment graphs, you can use kubectl to create the cluster and its graphs. In-place update for Ray Serve deployment graph. Users can update the Ray Serve deployment graph config in the RayService CR config and use kubectl apply to update the deployment graph. Zero downtime upgrade for Ray clusters. Users can update the Ray cluster config in the RayService CR config and use kubectl apply to update the cluster. RayService will temporarily create a pending cluster and wait for it to be ready, then switch traffic to the new cluster and terminate the old one. Services HA. RayService will monitor the Ray cluster and Serve deployments' health statuses. If RayService detects an unhealthy status for a period of time, RayService will try to create a new Ray cluster and switch traffic to the new cluster when it is ready.","title":"What does the RayService provide?"},{"location":"guidance/rayservice/#deploy-the-operator","text":"$ kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.3.0&timeout=90s\" Check that the controller is running. $ kubectl get deployments -n ray-system NAME READY UP-TO-DATE AVAILABLE AGE ray-operator 1 /1 1 1 40s $ kubectl get pods -n ray-system NAME READY STATUS RESTARTS AGE ray-operator-75dbbf8587-5lrvn 1 /1 Running 0 31s","title":"Deploy the Operator"},{"location":"guidance/rayservice/#run-an-example-cluster","text":"An example config file to deploy RayService is included here: ray_v1alpha1_rayservice.yaml # Create a ray service and deploy fruit deployment graph. $ kubectl apply -f config/samples/ray_v1alpha1_rayservice.yaml # List running RayServices. $ kubectl get rayservice NAME AGE rayservice-sample 7s # The created RayService should include a head pod, a worker pod, and four services. $ kubectl get pods NAME READY STATUS RESTARTS AGE ervice-sample-raycluster-qd2vl-worker-small-group-bxpp6 1 /1 Running 0 24m rayservice-sample-raycluster-qd2vl-head-45hj4 1 /1 Running 0 24m $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .100.0.1 <none> 443 /TCP 62d # A head node service maintained by the RayService. rayservice-sample-head-svc ClusterIP 10 .100.34.24 <none> 6379 /TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP 24m # A dashboard agent service maintained by the RayCluster. rayservice-sample-raycluster-qd2vl-dashboard-svc ClusterIP 10 .100.109.177 <none> 52365 /TCP 24m # A head node service maintained by the RayCluster. rayservice-sample-raycluster-qd2vl-head-svc ClusterIP 10 .100.180.221 <none> 6379 /TCP,8265/TCP,10001/TCP,8000/TCP,52365/TCP 24m # A serve service maintained by the RayService. rayservice-sample-serve-svc ClusterIP 10 .100.39.92 <none> 8000 /TCP 24m Note: Default ports and their definitions. Port Definition 6379 Ray GCS 8265 Ray Dashboard 10001 Ray Client 8000 Ray Serve 52365 Ray Dashboard Agent Get information about the RayService using its name. $ kubectl describe rayservices rayservice-sample","title":"Run an Example Cluster"},{"location":"guidance/rayservice/#access-user-services","text":"The users' traffic can go through the serve service (e.g. rayservice-sample-serve-svc ).","title":"Access User Services"},{"location":"guidance/rayservice/#run-a-curl-pod","text":"$ kubectl run curl --image = radial/busyboxplus:curl -i --tty Or if you already have a curl pod running, you can login using kubectl exec -it curl sh . For the fruit example deployment, you can try the following request: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000 -d '[\"MANGO\", 2]' > 6 You should get the response 6 .","title":"Run a Curl Pod"},{"location":"guidance/rayservice/#use-port-forwarding","text":"Set up Kubernetes port forwarding. $ kubectl port-forward service/rayservice-sample-serve-svc 8000 For the fruit example deployment, you can try the following request: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' localhost:8000 -d '[\"MANGO\", 2]' > 6 Note: serve-svc is HA in general. It will do traffic routing among all the workers which have serve deployments and will always try to point to the healthy cluster, even during upgrading or failing cases. You can set serviceUnhealthySecondThreshold to define the threshold of seconds that the serve deployments fail. You can also set deploymentUnhealthySecondThreshold to define the threshold of seconds that Ray fails to deploy any serve deployments.","title":"Use Port Forwarding"},{"location":"guidance/rayservice/#access-ray-dashboard","text":"Set up Kubernetes port forwarding for the dashboard. $ kubectl port-forward service/rayservice-sample-head-svc 8265 Access the dashboard using a web browser at localhost:8265 .","title":"Access Ray Dashboard"},{"location":"guidance/rayservice/#update-ray-serve-deployment-graph","text":"You can update the serveConfig in your RayService config file. For example, update the price of mangos to 4 in ray_v1alpha1_rayservice.yaml : - name: MangoStand numReplicas: 1 userConfig: | price: 4 Use kubectl apply to update your RayService and kubectl describe rayservices rayservice-sample to take a look at the RayService's information. It should look similar to: serveDeploymentStatuses: - healthLastUpdateTime: \"2022-07-18T21:51:37Z\" lastUpdateTime: \"2022-07-18T21:51:41Z\" name: MangoStand status: UPDATING After it finishes deployment, let's send a request again. In the curl pod from earlier, run: [ root@curl:/ ] $ curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc.default.svc.cluster.local:8000 -d '[\"MANGO\", 2]' > 8 Or if using port forwarding: curl -X POST -H 'Content-Type: application/json' localhost:8000 -d '[\"MANGO\", 2]' > 8 You should now get 8 as a result.","title":"Update Ray Serve Deployment Graph"},{"location":"guidance/rayservice/#upgrade-rayservice-raycluster-config","text":"You can update the rayClusterConfig in your RayService config file. For example, you can increase the number of workers to 2: workerGroupSpecs: # the pod replicas in this group typed worker - replicas: 2 Use kubectl apply to update your RayService and kubectl describe rayservices rayservice-sample to take a look at the RayService's information. It should look similar to: pendingServiceStatus: appStatus: {} dashboardStatus: healthLastUpdateTime: \"2022-07-18T21:54:53Z\" lastUpdateTime: \"2022-07-18T21:54:54Z\" rayClusterName: rayservice-sample-raycluster-bshfr rayClusterStatus: {} You can see the RayService is preparing a pending cluster. Once the pending cluster is healthy, the RayService will make it the active cluster and terminate the previous one.","title":"Upgrade RayService RayCluster Config"},{"location":"guidance/rayservice/#rayservice-observability","text":"You can use kubectl logs to check the operator logs or the head/worker nodes logs. You can also use kubectl describe rayservices rayservice-sample to check the states and event logs of your RayService instance. For Ray Serve monitoring, you can refer to the Ray observability documentation . To run Ray state APIs, log in to the head pod by running kubectl exec -it <head-node-pod> bash and use the Ray CLI or you can run commands locally using kubectl exec -it <head-node-pod> -- <ray state api> . For example, kubectl exec -it <head-node-pod> -- ray summary tasks outputs the following: ======== Tasks Summary: 2022 -07-28 15 :10:24.801670 ======== Stats: ------------------------------------ total_actor_scheduled: 17 total_actor_tasks: 5 total_tasks: 0 Table ( group by func_name ) : ------------------------------------ FUNC_OR_CLASS_NAME STATE_COUNTS TYPE 0 ServeController.listen_for_change RUNNING: 5 ACTOR_TASK 1 ServeReplica:MangoStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 2 HTTPProxyActor.__init__ FINISHED: 2 ACTOR_CREATION_TASK 3 ServeReplica:PearStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 4 ServeReplica:OrangeStand.__init__ FINISHED: 3 ACTOR_CREATION_TASK 5 ServeReplica:FruitMarket.__init__ FINISHED: 3 ACTOR_CREATION_TASK 6 ServeReplica:DAGDriver.__init__ FINISHED: 2 ACTOR_CREATION_TASK 7 ServeController.__init__ FINISHED: 1 ACTOR_CREATION_TASK","title":"RayService Observability"},{"location":"guidance/rayservice/#delete-the-rayservice-instance","text":"$ kubectl delete -f config/samples/ray_v1alpha1_rayservice.yaml","title":"Delete the RayService Instance"},{"location":"guidance/rayservice/#delete-the-operator","text":"$ kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default\"","title":"Delete the Operator"},{"location":"release/helm-chart/","text":"Helm charts release \u00b6 We host all Helm charts on kuberay-helm . This document describes the process for release managers to release Helm charts. The end-to-end workflow \u00b6 Step 1: Update version in Chart.yaml \u00b6 Please update the value of version in ray-cluster/Chart.yaml , kuberay-operator/Chart.yaml , and kuberay-apiserver/Chart.yaml to the new release version (e.g. 0.4.0). Step 2: Copy the helm-chart directory from kuberay to kuberay-helm \u00b6 In kuberay-helm CI , helm/chart-releaser-action will create releases for all charts in the directory helm-chart and update index.yaml in the gh-pages branch when the PR is merged into main . Note that index.yaml is necessary when you run the command helm repo add . I recommend removing the helm-chart directory in the kuberay-helm repository and creating a new one by copying from the kuberay repository. Step 3: Check the correctness \u00b6 When the PR is merged into main , the releases and index.yaml will be generated. You can check the correctness by: Check whether the releases are created as expectation. Check whether index.yaml exists or not. Check whether index.yaml has metadata of all releases, including old versions. Check the creation/update time of all releases and index.yaml to ensure they are updated. Install charts from Helm repository. helm repo add kuberay https://ray-project.github.io/kuberay-helm/ # List all charts helm search repo kuberay # Install charts helm install kuberay-operator kuberay/kuberay-operator helm install kuberay-apiserver kuberay/kuberay-apiserver helm install ray-cluster kuberay/ray-cluster Delete the existing releases \u00b6 helm/chart-releaser-action does not encourage users to delete existing releases; thus, index.yaml will not be updated automatically after the deletion. If you really need to do that, please read this section carefully before you do that. Delete the releases Remove the related tags by the following command. If tags are not properly removed, ray-project/kuberay/#561 may occur. # git remote -v # upstream git@github.com:ray-project/kuberay-helm.git (fetch) # upstream git@github.com:ray-project/kuberay-helm.git (push) # The following command deletes the tag \"ray-cluster-0.4.0\". git push --delete upstream ray-cluster-0.4.0 * Remove index.yaml * Trigger kuberay-helm CI again to create new releases and new index.yaml. * Follow \"Step3: Check the correctness\" to test it.","title":"Helm charts release"},{"location":"release/helm-chart/#helm-charts-release","text":"We host all Helm charts on kuberay-helm . This document describes the process for release managers to release Helm charts.","title":"Helm charts release"},{"location":"release/helm-chart/#the-end-to-end-workflow","text":"","title":"The end-to-end workflow"},{"location":"release/helm-chart/#step-1-update-version-in-chartyaml","text":"Please update the value of version in ray-cluster/Chart.yaml , kuberay-operator/Chart.yaml , and kuberay-apiserver/Chart.yaml to the new release version (e.g. 0.4.0).","title":"Step 1: Update version in Chart.yaml"},{"location":"release/helm-chart/#step-2-copy-the-helm-chart-directory-from-kuberay-to-kuberay-helm","text":"In kuberay-helm CI , helm/chart-releaser-action will create releases for all charts in the directory helm-chart and update index.yaml in the gh-pages branch when the PR is merged into main . Note that index.yaml is necessary when you run the command helm repo add . I recommend removing the helm-chart directory in the kuberay-helm repository and creating a new one by copying from the kuberay repository.","title":"Step 2: Copy the helm-chart directory from kuberay to kuberay-helm"},{"location":"release/helm-chart/#step-3-check-the-correctness","text":"When the PR is merged into main , the releases and index.yaml will be generated. You can check the correctness by: Check whether the releases are created as expectation. Check whether index.yaml exists or not. Check whether index.yaml has metadata of all releases, including old versions. Check the creation/update time of all releases and index.yaml to ensure they are updated. Install charts from Helm repository. helm repo add kuberay https://ray-project.github.io/kuberay-helm/ # List all charts helm search repo kuberay # Install charts helm install kuberay-operator kuberay/kuberay-operator helm install kuberay-apiserver kuberay/kuberay-apiserver helm install ray-cluster kuberay/ray-cluster","title":"Step 3: Check the correctness"},{"location":"release/helm-chart/#delete-the-existing-releases","text":"helm/chart-releaser-action does not encourage users to delete existing releases; thus, index.yaml will not be updated automatically after the deletion. If you really need to do that, please read this section carefully before you do that. Delete the releases Remove the related tags by the following command. If tags are not properly removed, ray-project/kuberay/#561 may occur. # git remote -v # upstream git@github.com:ray-project/kuberay-helm.git (fetch) # upstream git@github.com:ray-project/kuberay-helm.git (push) # The following command deletes the tag \"ray-cluster-0.4.0\". git push --delete upstream ray-cluster-0.4.0 * Remove index.yaml * Trigger kuberay-helm CI again to create new releases and new index.yaml. * Follow \"Step3: Check the correctness\" to test it.","title":"Delete the existing releases"}]}